---
title: "Initial NLP Work on Film Scripts"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Last updated: `r format(Sys.time(), '%B %d, %Y')`"
editor: source
execute:
  message: false
  warning: false
  error: false
format:
  html:
    theme:
      light: cerulean
      dark: cyborg
    anchor-sections: true
    embed-resources: true
    number-sections: true
    smooth-scroll: true
    toc: true
    toc-depth: 3
    toc-location: left
    code-fold: true
    code-summary: "Show code"
    fig-width: 11
    fig-height: 8
---


```{r import_libraries}
#| include: false

library(conflicted)
library(tidyverse)
library(magrittr)
library(rlang)
library(fs)
library(purrr)
library(furrr)
library(glue)
library(cowplot)
library(tidytext)
library(quanteda)
library(ggwordcloud)
library(SnowballC)
library(hunspell)



source("lib_utils.R")


conflict_lst <- resolve_conflicts(
  c("magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2")
  )

options(
  width = 80L,
  warn  = 1,
  mc.cores = parallelly::availableCores()
  )

set.seed(42)

theme_set(theme_cowplot())
```

In this workbook we perform the initial NLP pre-processing and simple
explorations and visualisations of the data.

# Load Data

```{r load_films_master_data}
#| echo: true

films_master_tbl <- read_rds("data/films_master_tbl.rds")

films_master_tbl |> glimpse()
```

The parsed data is stored in the file listed in the column `parsed_file` and
contains to separate tibbles, one with the detailed parsing of the script and
the second which aggregates all the text for both scene directions and
dialogue.

Also, a number of film scripts have not parsed properly, so we also want to
create a list of those films and exclude them from the analysis.

We may go back later to improve the parsing and if this happens we will updated
this list.

```{r create_film_exclusion_list}
#| echo: true

films_exclude_tbl <- c(
    "12_years_a_slave", "2001_a_space_odyssey", "django_unchained",
    "donnie_brasco", "drive", "gran_torino", "leaving_las_vegas",
    "lock_stock_and_two_smoking_barrels", "moneyball", "office_space",
    "star_wars_return_of_the_jedi", "the_green_mile"
    ) |>
  enframe(name = NULL, value = "title_cleaned")

films_exclude_tbl |> glimpse()
```



```{r load_parsed_script_data}
#| echo: true

films_parsed_tbl <- films_master_tbl |>
  anti_join(films_exclude_tbl, by = "title_cleaned") |>
  mutate(
    parsed_data = map(parsed_file, read_rds)
    ) |>
  unnest(parsed_data) |>
  select(
    film_title, release_year, genre, title_cleaned, parsing_detailed,
    parsing_aggregated
    )

films_parsed_tbl |> glimpse()
```



# Initial NLP Processing

We now want to perform some very basic NLP processing such as *tokenisation*.

Once we have tokenised the script, we also remove "stop words" - that is, common
words that do not convey meaning, such as "and", "to", "the" and so on.



```{r process_text_initial_nlp}
#| echo: true
data(stop_words)

films_tokens_tbl <- films_parsed_tbl |>
  mutate(
    wordtoken_data = map(
      parsing_aggregated, unnest_tokens,
      output = word, input = trimmed_text
      ),
    ngramtoken_data = map(
      parsing_aggregated, unnest_tokens,
      output = word, input = trimmed_text, token = "ngrams", n = 2, n_min = 1
      )
    ) |>
  select(-parsing_detailed, -parsing_aggregated)

films_wordtoken_unstopped_tbl <- films_tokens_tbl |>
  select(-ngramtoken_data) |>
  unnest(wordtoken_data) |>
  select(-full_text)

films_wordtoken_tbl <- films_wordtoken_unstopped_tbl |>
  anti_join(stop_words, by = "word")
```


## Show Initial Wordclouds

We now want to create some word clouds as a quick initial visualisation of the
data.

```{r plot_data_wordclouds_unstopped}
#| echo: true

plot_unstopped_tbl <- films_wordtoken_unstopped_tbl |>
  count(word) |>
  slice_max(order_by = n, n = 500)

ggwordcloud2(plot_unstopped_tbl, size = 4, seed = 421)
```


```{r plot_data_wordclouds_stopped}
#| echo: false

plot_stopped_tbl <- films_wordtoken_tbl |>
  count(word) |>
  slice_max(order_by = n, n = 500)

ggwordcloud2(plot_stopped_tbl, size = 2, seed = 422)
```


## Word-stemming

We also want to look at stemming our words.

```{r add_word_stems}
#| echo: true

films_stems_tbl <- films_wordtoken_tbl |>
  mutate(
    snowball_stem = wordStem(word),
    hunspell_stem = hunspell_stem(word)
    )

films_stems_tbl |> glimpse()
```

We also create a word cloud of this stemmed data

```{r create_stemmed_word_cloud}
#| echo: true

plot_stemmed_tbl <- films_stems_tbl |>
  count(word = snowball_stem) |>
  slice_max(order_by = n, n = 500)

ggwordcloud2(plot_stemmed_tbl, size = 2, seed = 422)
```


## Contrasting Dialogue and Direction

Finally we look just at the words in the lines of dialogue and focus on this.

```{r create_dialogue_word_cloud}
#| echo: true

plot_dialogue_tbl <- films_wordtoken_tbl |>
  filter(flag_dialogue == TRUE) |>
  count(word) |>
  slice_max(order_by = n, n = 500)

ggwordcloud2(plot_stemmed_tbl, size = 2, seed = 422)
```


# Sentiment Analysis

Sentiment analysis takes the simple approach of assigning some kind of measure
of sentiment or emotion to each word, allowing us to quantify these concepts
in the text in various ways.

Note that this approach is simplistic: it does not consider context or anything
beyond the presence of each word, but it is a quick and simple thing to look at.

There are a number of different sets of sentiment data, so we 

```{r retrieve_sentiments}
#| echo: true

sentiments_afinn_tbl    <- get_sentiments("afinn")
sentiments_afinn_tbl    |> glimpse()

sentiments_bing_tbl     <- get_sentiments("bing")
sentiments_bing_tbl     |> glimpse()

sentiments_loughran_tbl <- get_sentiments("loughran")
sentiments_loughran_tbl |> glimpse()

sentiments_nrc_tbl      <- get_sentiments("nrc")
sentiments_nrc_tbl      |> glimpse()
```


## Visualising the NRC Sentiments

We use the NRC sentiments and the count the appearance of each emotion in this
dataset.

```{r plot_nrc_sentiments_raw_count}
#| echo: true

plot_sentiments_tbl <- films_wordtoken_tbl |>
  inner_join(sentiments_nrc_tbl, by = "word") |>
  count(title_cleaned, sentiment)

ggplot(plot_sentiments_tbl) +
  geom_tile(
    aes(x = title_cleaned %>% str_trunc(20), y = sentiment, fill = n)
    ) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(
    x = "Film Title",
    y = "Sentiment",
    fill = "Raw Count",
    title = "Sentiments in Film Scripts"
    ) +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.5))
```

Raw counts are interesting, but it is also worth looking at scaling these
counts by the total word count of the script, and then plot each of those
counts as a ratio of the total word count in the script.

```{r plot_nrc_sentiments_ratio}
#| echo: true

films_wordcount_tbl <- films_wordtoken_tbl |>
  count(title_cleaned, name = "total_count")

plot_sentiments_ratio_tbl <- films_wordtoken_tbl |>
  inner_join(sentiments_nrc_tbl, by = "word") |>
  count(title_cleaned, sentiment, name = "word_count") |>
  inner_join(films_wordcount_tbl, by = "title_cleaned") |>
  mutate(word_ratio = word_count / total_count)
  
ggplot(plot_sentiments_ratio_tbl) +
  geom_tile(
    aes(x = title_cleaned %>% str_trunc(20), y = sentiment, fill = word_ratio)
    ) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(
    x = "Film Title",
    y = "Sentiment",
    fill = "Ratio",
    title = "Sentiments in Film Scripts"
    ) +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.5))

```



# R Environment

```{r show_session_info}
#| echo: false
#| message: false


options(width = 120L)
sessioninfo::session_info()
options(width = 80L)
```
