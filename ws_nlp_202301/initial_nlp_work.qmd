---
title: "Initial NLP Work on Film Scripts"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Last updated: `r format(Sys.time(), '%B %d, %Y')`"
editor: source
execute:
  message: false
  warning: false
  error: false
format:
  html:
    theme:
      light: cerulean
      dark: cyborg
    anchor-sections: true
    embed-resources: true
    number-sections: true
    smooth-scroll: true
    toc: true
    toc-depth: 3
    toc-location: left
    code-fold: true
    code-summary: "Show code"
    fig-width: 11
    fig-height: 8
---


```{r import_libraries}
#| include: false

library(conflicted)
library(tidyverse)
library(magrittr)
library(rlang)
library(fs)
library(purrr)
library(furrr)
library(glue)
library(cowplot)
library(tidytext)
library(quanteda)
library(ggwordcloud)
library(SnowballC)
library(hunspell)
library(DT)
library(igraph)
library(tidygraph)
library(ggraph)
library(ggnetwork)


source("lib_utils.R")


conflict_lst <- resolve_conflicts(
  c("magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2", "tidygraph")
  )

options(
  width = 80L,
  warn  = 1,
  mc.cores = parallelly::availableCores()
  )

set.seed(42)

theme_set(theme_cowplot())
```

In this workbook we perform the initial NLP pre-processing and simple
explorations and visualisations of the data.

# Load Data

```{r load_films_master_data}
#| echo: true

films_master_tbl <- read_rds("data/films_master_tbl.rds")

films_master_tbl |> glimpse()
```

The parsed data is stored in the file listed in the column `parsed_file` and
contains to separate tibbles, one with the detailed parsing of the script and
the second which aggregates all the text for both scene directions and
dialogue.


## Load Sentiments Data

We also want to load the data around the sentiments.

```{r retrieve_sentiments}
#| echo: true

sentiments_afinn_tbl    <- read_rds("input_data/sentiments_afinn_tbl.rds")
sentiments_afinn_tbl    |> glimpse()

sentiments_bing_tbl     <- read_rds("input_data/sentiments_bing_tbl.rds")
sentiments_afinn_tbl    |> glimpse()

sentiments_loughran_tbl <- read_rds("input_data/sentiments_loughran_tbl.rds")
sentiments_loughran_tbl |> glimpse()

sentiments_nrc_tbl      <- read_rds("input_data/sentiments_nrc_tbl.rds")
sentiments_nrc_tbl      |> glimpse()
```


## Pre-process Data

Also, a number of film scripts have not parsed properly, so we also want to
create a list of those films and exclude them from the analysis.

We may go back later to improve the parsing and if this happens we will updated
this list.

```{r create_film_exclusion_list}
#| echo: true

films_exclude_tbl <- c(
    "12_years_a_slave", "2001_a_space_odyssey", "django_unchained",
    "donnie_brasco", "drive", "gran_torino", "leaving_las_vegas",
    "lock_stock_and_two_smoking_barrels", "moneyball", "office_space",
    "star_wars_return_of_the_jedi", "the_green_mile"
    ) |>
  enframe(name = NULL, value = "title_cleaned")

films_exclude_tbl |> glimpse()
```


Having excluded a number of films from the parsing and future analysis we
load in the scripts data and read the data into our table.

```{r load_parsed_script_data}
#| echo: true

films_parsed_tbl <- films_master_tbl |>
  anti_join(films_exclude_tbl, by = "title_cleaned") |>
  mutate(
    parsed_data = map(parsed_file, read_rds)
    ) |>
  unnest(parsed_data) |>
  select(
    film_title, release_year, genre, title_cleaned, parsing_detailed,
    parsing_aggregated
    )

films_parsed_tbl |> glimpse()
```




# Initial NLP Processing

We now want to perform some very basic NLP processing such as *tokenisation*.

Once we have tokenised the script, we also remove "stop words" - that is, common
words that do not convey meaning, such as "and", "to", "the" and so on.



```{r process_text_initial_nlp}
#| echo: true
data(stop_words)

films_tokens_tbl <- films_parsed_tbl |>
  mutate(
    wordtoken_data = map(
      parsing_aggregated, unnest_tokens,
      output = word, input = trimmed_text
      ),
    ngramtoken_data = map(
      parsing_aggregated, unnest_tokens,
      output = word, input = trimmed_text, token = "ngrams", n = 2, n_min = 1
      )
    ) |>
  select(-parsing_detailed, -parsing_aggregated)

films_wordtoken_unstopped_tbl <- films_tokens_tbl |>
  select(-ngramtoken_data) |>
  unnest(wordtoken_data) |>
  select(-full_text)

films_wordtoken_tbl <- films_wordtoken_unstopped_tbl |>
  anti_join(stop_words, by = "word")
```


## Show Initial Wordclouds

We now want to create some word clouds as a quick initial visualisation of the
data.

```{r plot_data_wordclouds_unstopped}
#| echo: true

plot_unstopped_tbl <- films_wordtoken_unstopped_tbl |>
  count(word) |>
  slice_max(order_by = n, n = 500)

ggwordcloud2(plot_unstopped_tbl, size = 4, seed = 421)
```


```{r plot_data_wordclouds_stopped}
#| echo: false

plot_stopped_tbl <- films_wordtoken_tbl |>
  count(word) |>
  slice_max(order_by = n, n = 500)

ggwordcloud2(plot_stopped_tbl, size = 2, seed = 422)
```


## Word-stemming

We also want to look at stemming our words.

```{r add_word_stems}
#| echo: true

films_stems_tbl <- films_wordtoken_tbl |>
  mutate(
    snowball_stem = wordStem(word),
    hunspell_stem = hunspell_stem(word)
    )

films_stems_tbl |> glimpse()
```

We also create a word cloud of this stemmed data

```{r create_stemmed_word_cloud}
#| echo: true

plot_stemmed_tbl <- films_stems_tbl |>
  count(word = snowball_stem) |>
  slice_max(order_by = n, n = 500)

ggwordcloud2(plot_stemmed_tbl, size = 2, seed = 422)
```


## Contrasting Dialogue and Direction

Finally we look just at the words in the lines of dialogue and focus on this.

```{r create_dialogue_word_cloud}
#| echo: true

plot_dialogue_tbl <- films_wordtoken_tbl |>
  filter(flag_dialogue == TRUE) |>
  count(word) |>
  slice_max(order_by = n, n = 500)

ggwordcloud2(plot_stemmed_tbl, size = 2, seed = 422)
```


# Sentiment Analysis

Sentiment analysis takes the simple approach of assigning some kind of measure
of sentiment or emotion to each word, allowing us to quantify these concepts
in the text in various ways.

Note that this approach is simplistic: it does not consider context or anything
beyond the presence of each word, but it is a quick and simple thing to look at.

There are a number of different sets of sentiment data, so we 



## Visualising the NRC Sentiments

We use the NRC sentiments and the count the appearance of each emotion in this
dataset.

```{r plot_nrc_sentiments_raw_count}
#| echo: true

sentiments_nrc_tbl |>
  datatable(
    rownames = FALSE,
    caption  = "nrc Sentiments"
    )


plot_sentiments_tbl <- films_wordtoken_tbl |>
  inner_join(sentiments_nrc_tbl, by = "word") |>
  count(title_cleaned, sentiment)

ggplot(plot_sentiments_tbl) +
  geom_tile(
    aes(x = title_cleaned %>% str_trunc(20), y = sentiment, fill = n)
    ) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(
    x = "Film Title",
    y = "Sentiment",
    fill = "Raw Count",
    title = "Sentiments in Film Scripts"
    ) +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.5))
```

Raw counts are interesting, but it is also worth looking at scaling these
counts by the total word count of the script, and then plot each of those
counts as a ratio of the total word count in the script.

```{r plot_nrc_sentiments_ratio}
#| echo: true

films_wordcount_tbl <- films_wordtoken_tbl |>
  count(title_cleaned, name = "total_count")

plot_sentiments_ratio_tbl <- films_wordtoken_tbl |>
  inner_join(sentiments_nrc_tbl, by = "word") |>
  count(title_cleaned, sentiment, name = "word_count") |>
  inner_join(films_wordcount_tbl, by = "title_cleaned") |>
  mutate(word_ratio = word_count / total_count)
  
ggplot(plot_sentiments_ratio_tbl) +
  geom_tile(
    aes(x = title_cleaned %>% str_trunc(20), y = sentiment, fill = word_ratio)
    ) +
  scale_fill_gradient(low = "blue", high = "red") +
  labs(
    x = "Film Title",
    y = "Sentiment",
    fill = "Ratio",
    title = "Sentiments in Film Scripts"
    ) +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.5, size = 8))

```

## Visualising afinn Sentiments

We now repeat the above exercise, but using the sentiment words in the `afinn`
data - in this dataset each word is assigned a positive or negative number of
the degree of 'positivity' associated with the word.

```{r plot_afinn_sentiments}
#| echo: true

sentiments_afinn_tbl |>
  datatable(
    rownames = FALSE,
    caption  = "afinn Sentiments"
    )


plot_sentiments_tbl <- films_wordtoken_tbl |>
  inner_join(sentiments_afinn_tbl, by = "word") |>
  count(genre, title_cleaned, wt = value, name = "total_sentiment") |>
  inner_join(films_wordcount_tbl, by = "title_cleaned") |>
  mutate(sentiment_ratio = total_sentiment / total_count)


ggplot(plot_sentiments_tbl) +
  geom_col(
    aes(x = title_cleaned %>% str_trunc(20), y = total_sentiment, fill = genre)
    ) +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  labs(
    x = "Film Title",
    y = "Total Sentiment",
    fill = "Genre",
    title = "Total afinn Sentiments by Film"
    ) +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.5, size = 8))

ggplot(plot_sentiments_tbl) +
  geom_col(
    aes(x = title_cleaned %>% str_trunc(20), y = sentiment_ratio, fill = genre)
    ) +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  labs(
    x = "Film Title",
    y = "Sentiment Ratio",
    fill = "Genre",
    title = "Ratio of afinn Sentiments by Total Word Count by Film"
    ) +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.5, size = 8))

```


We now want to look at the distribution of sentiment measures in each film.

```{r plot_afinn_sentiment_distribution_by_title}
#| echo: true

plot_sentiments_tbl <- films_wordtoken_tbl |>
  inner_join(sentiments_afinn_tbl, by = "word") |>
  count(genre, title_cleaned, value, name = "value_count") |>
  inner_join(films_wordcount_tbl, by = "title_cleaned") |>
  mutate(
    sentiment_ratio = value_count / total_count
    )

ggplot(plot_sentiments_tbl) +
  geom_col(aes(x = value, y = sentiment_ratio)) +
  facet_wrap(vars(title_cleaned), scales = "free_y") +
  labs(
    x = "Sentiment Value",
    y = "Sentiment Ratio",
    title = "Facet Plot of Distribution of Sentiment Values"
    ) +
  theme(strip.text.x = element_text(size = 8))
```


## Visualising bing Sentiments

The `bing` dataset is a list of words with positive and negative sentiment.

```{r plot_bing_sentiments}
#| echo: true

sentiments_bing_tbl |>
  datatable(
    rownames = FALSE,
    caption  = "bing Sentiments"
    )


plot_sentiments_tbl <- films_wordtoken_tbl |>
  inner_join(sentiments_bing_tbl, by = "word") |>
  count(genre, title_cleaned, sentiment, name = "sentiment_count") |>
  inner_join(films_wordcount_tbl, by = "title_cleaned") |>
  mutate(
    sentiment_ratio = sentiment_count / total_count
    )


ggplot(plot_sentiments_tbl) +
  geom_col(
    aes(x = title_cleaned %>% str_trunc(20), y = sentiment_ratio, fill = sentiment),
    position = "dodge"
    ) +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  labs(
    x = "Film Title",
    y = "Sentiment Ratio",
    fill = "Sentiment",
    title = "Total bing Sentiment by Film"
    ) +
  theme(axis.text.x = element_text(angle = 20, vjust = 0.5, size = 8))

```


## Visualising loughran Sentiments

The `loughran` dataset is a list of words with positive and negative sentiment,
similar to the `bing` dataset.


```{r plot_loughran_sentiments}
#| echo: true

sentiments_loughran_tbl |>
  datatable(
    rownames = FALSE,
    caption  = "loughran Sentiments"
    )


plot_sentiments_tbl <- films_wordtoken_tbl |>
  inner_join(sentiments_loughran_tbl, by = "word") |>
  count(genre, title_cleaned, sentiment, name = "sentiment_count") |>
  inner_join(films_wordcount_tbl, by = "title_cleaned") |>
  mutate(
    sentiment_ratio = sentiment_count / total_count
    )


ggplot(plot_sentiments_tbl) +
  geom_col(
    aes(x = title_cleaned %>% str_trunc(20), y = sentiment_ratio, fill = sentiment),
    position = "dodge"
    ) +
  scale_fill_brewer(type = "qual", palette = "Set1") +
  labs(
    x = "Film Title",
    y = "Sentiment Ratio",
    fill = "Sentiment",
    title = "Total loughran Sentiment by Film"
    ) +
  theme(axis.text.x = element_text(angle = 30, vjust = 0.5, size = 8))


ggplot(plot_sentiments_tbl) +
  geom_col(aes(x = title_cleaned %>% str_trunc(20), y = sentiment_ratio)) +
  facet_wrap(vars(sentiment), scales = "free_y") +
  labs(
    x = "Film Title",
    y = "Sentiment Ratio",
    title = "Facet Plot of Distribution of Sentiment Values"
    ) +
  theme(
    axis.text.x = element_text(angle = 30, vjust = 0.5, size = 4),
    strip.text.x = element_text(size = 8)
    )
```


# Word and Document Frequency

We now want to look at the use of words within each film and overall as well.

In particular, we also account for differences between stage directions and
dialogue, and so we also want to analyse the word tokens without excluding
any stop words.

```{r plot_word_rank_frequency_plots}
#| echo: true

total_wordfreq_tbl <- films_wordtoken_unstopped_tbl |>
  count(word, sort = TRUE) |>
  mutate(
    freq = n / sum(n),
    rank = row_number()
    )

ggplot(total_wordfreq_tbl) +
  geom_line(aes(x = rank, y = freq)) +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    x = "Word Rank",
    y = "Word Frequency",
    title = "Log-Log Plot of Word Frequency vs Ranking"
    )
```

Overall, we see that word frequency is following a power-law, and it is worth
exploring differences if we segment by individual film.

```{r plot_films_word_rank_frequency_plots}
#| echo: true

total_film_wordfreq_tbl <- films_wordtoken_unstopped_tbl |>
  count(title_cleaned, word, sort = TRUE) |>
  group_by(title_cleaned) |>
  mutate(
    freq = n / sum(n),
    rank = row_number()
    )

ggplot(total_film_wordfreq_tbl) +
  geom_line(aes(x = rank, y = freq, colour = title_cleaned)) +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    x = "Word Rank",
    y = "Word Frequency",
    title = "Log-Log Plot of Film Word Frequency vs Ranking"
    ) +
  theme(legend.position = "none")
```

## Calculate Term Frequency - Inverse Document Frequency

We now want to look at the statistic known as the
*term frequency - inverse document frequency*, the TF-IDF. This statistic
calculates the relative frequency of each token in the corpus but then scales
this by the inverse of its frequency of appearance in each document.

The effect of this is to show terms that appear frequently in only a subset of
the documents - if a token appears in most or all of the documents, it is
heavily downweighted by its high document frequency.

In terms of defining the idea of a 'document' in this, we start by
considering each separate film to be a document.

```{r calculate_film_tf_idf}
#| echo: true

total_film_tfidf_tbl <- films_wordtoken_unstopped_tbl |>
  count(title_cleaned, word, name = "word_count") |>
  bind_tf_idf(word, title_cleaned, word_count)

total_film_tfidf_tbl |>
  slice_max(order_by = tf_idf, n = 500) |>
  datatable(
    rownames = FALSE,
    caption  = "TF-IDF Statistics for Words"
    )
```

We see that for each film we have a high TF-IDF for a token that appears to be
a character name in the film, so it is worth redoing this, but only looking
at the dialogue text.

This requires a bit of processing of the data, as we want to collapse all
the dialogue for a given character into a single document.

```{r collapse_script_text_to_character_dialogue}
#| echo: true

films_dialogue_text_tbl <- films_parsed_tbl |>
  select(-parsing_detailed) |>
  unnest(parsing_aggregated) |>
  select(-full_text) |>
  transmute(
    film_title, release_year, genre, title_cleaned,
    section_cleaned = section_title |>
      str_replace_all("\\(.*?\\)", "") |>
      str_squish(),
    flag_dialogue,
    grouping_id,
    trimmed_text
    ) |>
  filter(flag_dialogue == TRUE) |>
  group_by(film_title, release_year, genre, title_cleaned, section_cleaned) |>
  arrange(grouping_id) |>
  summarise(
    .groups = "drop",
    
    dialogue_text = str_c(trimmed_text, collapse = " ") |> str_squish()
    )

films_dialogue_text_tbl |>
  slice_head(n = 100) |>
  datatable(
    rownames = FALSE,
    caption  = "Film Dialogue Compressed - Character as Document"
  )
```


We first want to create a token table from this new format of data.

```{r create_film_dialogue_wordtoken}
#| echo: true

films_dialogue_wordtoken_tbl <- films_dialogue_text_tbl |>
  unnest_tokens(word, dialogue_text)

films_dialogue_wordtoken_tfidf_tbl <- films_dialogue_wordtoken_tbl |>
  count(title_cleaned, word, name = "word_count") |>
  bind_tf_idf(word, title_cleaned, word_count)

films_dialogue_wordtoken_tfidf_tbl |>
  slice_max(order_by = tf_idf, n = 500) |>
  datatable(
    rownames = FALSE,
    caption  = "Film Dialogue - TF-IDF Words"
    )
```

Despite filtering out the non-dialogue text, we are seeing very similar results
for the TF-IDF values in the dataset. As many of them are names, this still
makes sense, as it is likely that names are used as a part of the dialogue.


## Work on Bi-Gram Data

We now repeat this process, but now look at the bi-grams.

```{r display_bigram_data}
#| echo: true

films_ngrams_tbl <- films_tokens_tbl |>
  select(-wordtoken_data) |>
  unnest(ngramtoken_data)

films_ngrams_tbl |>
  drop_na(word) |>
  select(-full_text) |>
  slice_head(n = 500) |>
  datatable(
    rownames = FALSE,
    caption  = "ngram Tokens"
    )
```

We now want to look at the data and remove the stopwords where either of the
words in the n-gram is on the list.


```{r create_ngrams_stop_words_data}
#| echo: true

films_ngrams_stopped_tbl <- films_ngrams_tbl |>
  drop_na(word) |>
  mutate(
    token_word = word
    ) |>
  separate(token_word, c("word1", "word2"), sep = " ") |>
  anti_join(stop_words, by = c("word1" = "word")) |>
  anti_join(stop_words, by = c("word2" = "word")) |>
  select(-word1, -word2)

films_ngrams_tfidf_tbl <- films_ngrams_stopped_tbl |>
  count(title_cleaned, word, name = "word_count") |>
  bind_tf_idf(word, title_cleaned, word_count)

films_ngrams_tfidf_tbl |>
  slice_max(order_by = tf_idf, n = 500) |>
  datatable(
    rownames = FALSE,
    caption  = "Film Dialogue - TF-IDF Words"
    )
```



## Construct Graph Based on Bi-Grams

An alternative approach to looking at this data is to construct a directed
graph of words with the edges being determined by the first and second word of
the bi-gram.


```{r construct_bigram_graph}
#| echo: true

bigram_graph_edges_tbl <- films_ngrams_stopped_tbl |>
  filter(flag_dialogue == TRUE) |>
  separate(word, c("word1", "word2"), sep = " ") |>
  drop_na(word2) |>
  count(word1, word2, name = "bigram_count", sort = TRUE) |>
  filter(word1 != word2)

bigram_graph <- bigram_graph_edges_tbl |>
  graph_from_data_frame()

bigram_tblgraph <- bigram_graph |>
  as_tbl_graph() |>
  activate(nodes) |>
  mutate(
    comp_id = group_components()
    ) |>
  group_by(comp_id) |>
  mutate(
    comp_size = n()
    ) |>
  ungroup()

bigram_tblgraph |> print()
```

To help visualise this graph, we will look at a few of the smaller disjoint
components of this chart.

```{r plot_word_graph_components}
#| echo: true

bigram_tblgraph |>
  filter(comp_id %in% c(2, 3, 4, 5, 6, 7)) |>
  plot()
```

We now want to run some community detection routines on that largest component
of the graph, so we run this now.

```{r wordgraph_determine_communities}
#| echo: true
#| cache: true

bigram_tblgraph <- bigram_tblgraph |>
  convert(to_subgraph, comp_id == 1) |>
  mutate(
    clust_id = group_walktrap(weights = bigram_count)
    )

bigram_tblgraph |> print()
```



# R Environment

```{r show_session_info}
#| echo: false
#| message: false


options(width = 120L)
sessioninfo::session_info()
options(width = 80L)
```
