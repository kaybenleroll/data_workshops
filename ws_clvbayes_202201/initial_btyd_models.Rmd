---
title: "Initial Exploration of the P/NBD Model"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Last updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  rmdformats::readthedown:
    toc_depth: 3
    use_bookdown: TRUE
    code_folding: hide
    fig_caption: TRUE

  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    number_sections: TRUE
    toc: TRUE
    toc_depth: 2
    toc_float:
      smooth_scroll: FALSE

  pdf_document: default
---


```{r import_libraries, echo=FALSE, message=FALSE}
library(conflicted)
library(tidyverse)
library(scales)
library(cowplot)
library(directlabels)
library(magrittr)
library(rlang)
library(fs)
library(purrr)
library(furrr)
library(glue)
library(CLVTools)
library(tidyquant)
library(rsample)
library(MASS)
library(fitdistrplus)
library(rstan)
library(brms)
library(posterior)
library(bayesplot)
library(tidybayes)


source("lib_utils.R")

conflict_lst <- resolve_conflicts(
  c("magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2", "MASS",
    "fitdistrplus")
  )


knitr::opts_chunk$set(
  tidy       = FALSE,
  cache      = FALSE,
  warning    = FALSE,
  message    = FALSE,
  fig.height =     8,
  fig.width  =    11
  )

options(
  width = 80L,
  warn  = 1,
  mc.cores = parallel::detectCores()
  )

rstan_options(auto_write = FALSE)

theme_set(theme_cowplot())

set.seed(42)

plan(multisession)
```

In this workbook we introduce the various different BTYD models, starting with
a discussion of the underlying theory.


# Background Theory

Before we start working on fitting and using the various Buy-Till-You-Die
models, we first need to discuss the basic underlying theory and model.

In this model, we assume a customer becomes 'alive' to the business at the
first purchase and then makes purchases stochastically but at a steady-rate
for a period of time, and then 'dies' - i.e. becomes inactive to the business -
hence the use of "Buy-Till-You-Die".

Thus, at a high level these models decompose into modelling the transaction
events using distributions such as the Poisson or Negative Binomial, and then
modelling the 'dropout' process using some other method.

A number of BTYD models exist and for this workshop we will focus on the
BG/NBD model - the Beta-Geometric Negative Binomial Distribution model (though
we will discuss the P/NBD model also).

These models require only two pieces of information about each customer's
purchasing history: the "recency" (when the last transaction occurred) and
"frequency" (the count of transactions made by that customer in a specified
time period).

The notation used to represent this information is

$$
X = (x, \, t_x, \, T),
$$
where

$$
\begin{eqnarray*}
x   &=& \text{the number of transactions},           \\
T   &=& \text{the observed time period},             \\
t_x &=& \text{the time since the last transaction}.
\end{eqnarray*}
$$

From this summary data we can fit most BTYD models.

## BTYD Models

There are a number of different statistical approaches to building BTYD
models - relying on a number of different assumptions about how the various
recency, frequency and monetary values are modelled.

We now discuss a number of different ways of modelling this.


### Beta-Geometric/Negative-Binomial Distribution (BG/NBD) Model

This model relies on a number of base assumptions:

  1. While active, the number of transactions made by a customer follows a
  Poisson process with transaction rate $\lambda$.
  1. Heterogeneity in $\lambda$ follows a Gamma distribution
  $\Gamma(\lambda \, | \, \alpha, r)$ with parameters shape $r$ and rate
  $\alpha$. 
  1. After any transaction, a customer becomes inactive with probability $p$.
  1. Heterogeneity in $p$ follows a Beta distribution $B(p \, | \, a, b)$ with
  shape parameters $a$ and $b$.
  1. The transaction rate $\lambda$ and the dropout probability $p$ vary
  independently across customers.


Note that it follows from the above assumptions that the probability of a
customer being 'alive' after any transaction is given by the Geometric
distribution, and hence the Beta-Geometric in the name.

To put this into more formal mathematical notation, we have:
 
$$
\begin{eqnarray*}
\lambda &\sim& \Gamma(\alpha, r),                 \\
P(\text{alive}, k) &\sim& \text{Geometric}(p, k), \\
p &\sim& \text{Beta}(a, b)
\end{eqnarray*}
$$



### Pareto/Negative-Binomial Distribution (P/NBD) Model

Similar to the BG/NBD model, the P/NBD model relies on five assumptions:

  1. While active, the number of transactions made by a customer follows a
  Poisson process with transaction rate $\lambda$.
  1. Heterogeneity in $\lambda$ follows a Gamma distribution
  $\Gamma(\lambda \, | \, \alpha, r)$ with shape $r$ and rate $\alpha$. 
  1. Each customer has an unobserved 'lifetime' of length $\tau$. This point at
  which the customer becomes inactive is distributed as an exponential with
  dropout rate $\mu$.
  1. Heterogeneity in dropout rates across customers follows a Gamma
  distribution $\Gamma(\mu \, | \, s, \beta)$ with shape parameter $s$ and
  rate parameter $\beta$.
  1. The transaction rate $\lambda$ and the dropout rate $\mu$ vary
  independently across customers.


As before, we express this in mathematical notation as:

$$
\begin{eqnarray*}
\lambda &\sim& \Gamma(\alpha, r),    \\
\mu &\sim& \Gamma(s, \beta),         \\
\tau &\sim& \text{Exponential}(\mu)
\end{eqnarray*}
$$



### Gamma/Gamma Spending Model

The final piece of this approach is to construct a spending model for the
transactions. Similar to transaction rates, spending patterns are often
skewed, so a Gamma distribution is used.

For the Gamma/Gamma model, we use the following assumptions:

  1. Each transaction amount, $v_i$, is distributed according to a Gamma
  distribution $\Gamma(v_i \, | \, p, \nu)$ with shape parameter $p$ and
  rate parameter $\nu$.
  2. Heterogeneity in $nu$ follows a Gamma distribution
  $\Gamma(\nu \, | \, q, \gamma)$


In mathematical notation, we have:
 
$$
\begin{eqnarray*}
p &\sim& \Gamma(q, \gamma), \\
v &\sim& \Gamma(p, \nu)
\end{eqnarray*}
$$


# Generate Basic Synthetic Data

The above model contains a number of subtle statistical complexities, so we
do not dive straight into fitting real-world data. Instead, we use the model
to generate synthetic data and then fit this data back to the model, and
ensure we can recover our original parameters.


## Load Input Data

As a means of simplifying the process of generating synthetic data, and 
because we are not attempting to model the arrival of new customers, we will
use some summary data on the first transaction data of each customer as an
input to this process.

```{r load_customer_input_data, echo=TRUE}
customer_cohort_data_tbl <- read_rds("data/customer_cohort_tbl.rds")

customer_cohort_data_tbl %>% glimpse()
```

For the purposes of this work, we can treat the quarterly times as categorical
and so we convert these values to character strings.

```{r convert_cohort_qtr_values, echo=TRUE}
customer_cohort_data_tbl <- customer_cohort_data_tbl %>%
  transmute(
    customer_id,
    cohort_qtr = cohort_qtr %>% as.character(),
    cohort_ym,
    first_tnx_date
    )

customer_cohort_data_tbl %>% glimpse()
```


## Run Data Generation for Basic Model

We now want to set up the various parameters required for the data generation.

```{r setup_synthesis_parameters, echo=TRUE}
mu_shape <-  1
mu_rate  <- 10

lambda_shape <-  1
lambda_rate  <- 10

mx_p <- 100
mx_q <-   1
mx_g <-   1

final_date_observed <- customer_cohort_data_tbl %>%
  pull(first_tnx_date) %>%
  max()
```


Now that we have set our 'hyper-parameters' set for the data generation we
produce individual parameters for each of the customers.

```{r generate_individual_customer_basic_parameters, echo=TRUE}
customer_basic_parameters_tbl <- customer_cohort_data_tbl %>%
  select(customer_id, cohort_qtr, cohort_ym, first_tnx_date) %>%
  arrange(first_tnx_date, customer_id) %>%
  mutate(
    customer_mu     = rgamma(n(), shape = mu_shape,     rate = mu_rate),
    customer_tau    = rexp(n(), rate = customer_mu),
    customer_lambda = rgamma(n(), shape = lambda_shape, rate = lambda_rate),
    customer_nu     = rgamma(n(), shape = mx_q,         rate = mx_g)
    )

customer_basic_parameters_tbl %>% glimpse()
```

We now have set up the customer parameters for each customer and can use these
to simulate the various transactions on the website.

```{r generate_customer_transactions, echo=TRUE}
generate_customer_transactions <- function(lifetime, tnx_rate, mx_nu, mx_g,
                                           first_date, final_date = final_date) {
  
  obs_weeks <- difftime(final_date, first_date, units = "weeks") %>% as.numeric()
  
  tnx_window <- min(obs_weeks, lifetime)
  
  sim_count <- round(10 * (tnx_window * tnx_rate), 0) %>% max(10)
  
  event_times <- rexp(sim_count, rate = tnx_rate)

  use_event_weeks <- cumsum(event_times)
  use_event_weeks <- use_event_weeks[use_event_weeks < tnx_window]
  
  event_dates <- first_date + (use_event_weeks * 7)
  
  tnx_amounts <- rgamma(1 + length(event_dates), shape = mx_p, rate = mx_nu)
  
  sim_tnx_tbl <- tibble(
    tnx_date   = c(first_date, event_dates),
    tnx_amount = tnx_amounts %>% round(2)
    )
  
  return(sim_tnx_tbl)
}
```

We now run these data routines to generate the transaction data.

```{r generate_basic_dataset, echo=TRUE}
customer_basic_sims_tbl <- customer_basic_parameters_tbl %>%
  mutate(
    sim_data = pmap(
      list(
        lifetime   = customer_tau,
        tnx_rate   = customer_lambda,
        mx_nu      = customer_nu,
        mx_g       = mx_g,
        first_date = first_tnx_date
        ),
      generate_customer_transactions,
      final_date = final_date_observed
      )
    ) %>%
  select(
    customer_id, cohort_qtr, cohort_ym, sim_data
    ) %>%
  unnest(sim_data) %>%
  arrange(customer_id, tnx_date)

customer_basic_sims_tbl %>% glimpse()
```

## Write Data to Disk

We now write this data to disk.

```{r write_basic_data_disk, echo=TRUE}
customer_basic_parameters_tbl %>% write_rds("data/customer_basic_parameters_tbl.rds")

customer_basic_sims_tbl       %>% write_rds("data/customer_basic_sims_tbl.rds")
```


# Fit Synthetic Data Model

We take our transaction data and transform the data into a format amenable for
the BTYD models.

We first ensure that all transactions on the same day or combined into a single
transaction, and then convert this data into the $(x, t_x, T)$ triplet-format.

To mimic the training / test split to help validate real-world models, we do
the same thing here.

```{r set_training_data_date, echo=TRUE}
training_data_date <- as.Date("2011-03-31")
```

We now combine all this data to construct our CLV input data.


```{r construct_clvinput_data, echo=TRUE}
customer_basic_input_clvdata <- customer_basic_sims_tbl %>%
  count(customer_id, tnx_date, wt = tnx_amount, name = "total_spend") %>%
  arrange(customer_id, tnx_date) %>%
  left_join(customer_cohort_data_tbl, by = "customer_id") %>%
  filter(first_tnx_date <= training_data_date) %>%
  clvdata(
    date.format      = "%Y-%m-%d",
    time.unit        = "weeks",
    estimation.split = training_data_date,
    name.id          = "customer_id",
    name.date        = "tnx_date",
    name.price       = "total_spend"
    )

customer_basic_input_clvdata %>% glimpse()
```


Now that we have constructed the `clvdata` structure, we can use this input
to construct the summary statistics required for our models. This data is
performed by internal routines in `CLVTools`, so we use those directly.

```{r construct_pnbd_input_data, echo=TRUE}
customer_basic_pnbd_cbs_tbl <- customer_basic_input_clvdata %>%
  (CLVTools:::pnbd_cbs)() %>%
  rename(
    customer_id = Id,
    t_x         = t.x,
    T_cal       = T.cal
    )

customer_basic_pnbd_cbs_lst <- customer_basic_pnbd_cbs_tbl %>%
  select(-date.first.actual.trans) %>%
  compose_data(
    mu_shape     =  1,
    mu_rate      = 10,
  
    lambda_shape =  1,
    lambda_rate  = 10
    )

customer_basic_pnbd_cbs_lst %>% glimpse()
```


## Construct First Stan Model

```{r compile_basic_fixed_model, echo=TRUE, cache=TRUE}
pnbd_basic_fixed_stanmodel <- stan_model(
  "stan_code/clv_pnbd_fixed.stan",
  model_name    = "pnbd_basic_fixed_model",
  warn_pedantic =  TRUE,
  verbose       = FALSE
  )
```


```{r fit_basic_fixed_stan_model, echo=TRUE, warning=TRUE, cache=TRUE}
stan_data_lst <- compose_data(
  customer_basic_pnbd_cbs_lst,

  r     =  1,
  alpha = 10,
  
  s     =  1,
  beta  = 10,
  )

pnbd_basic_fixed_stanfit <- sampling(
  pnbd_basic_fixed_stanmodel,
  data   = stan_data_lst,
  chains = 4,
  iter   = 2000,
  seed   = 42
  )

pnbd_basic_fixed_stanfit %>% print()
```

We first want to check the HMC diagnostics.

```{r calculate_basic_fixed_hmc_diagnostics, echo=TRUE}
pnbd_basic_fixed_stanfit %>% check_hmc_diagnostics()

pnbd_basic_fixed_stanfit %>% get_bfmi() %>% round(4)
```


We use the functionality given by `tidybayes` to extract the posterior
distribution of the parameters.

```{r extract_basic_pnbd_posterior_params, echo=TRUE}
customer_basic_pnbd_posterior_tbl <- pnbd_basic_fixed_stanfit %>%
  recover_types(customer_basic_pnbd_cbs_tbl) %>%
  spread_draws(mu[customer_id], lambda[customer_id]) %>%
  ungroup()
```


```{r construct_basic_pnbd_posterior_validation, echo=TRUE}
customer_posterior_tbl <- customer_basic_pnbd_posterior_tbl %>%
  group_by(customer_id) %>%
  summarise(
    posterior_mu     = list(mu),
    posterior_lambda = list(lambda)
    )

customer_postcheck_tbl <- customer_basic_parameters_tbl %>%
  select(customer_id, customer_mu, customer_lambda, customer_tau) %>%
  inner_join(customer_posterior_tbl, by = "customer_id") %>%
  mutate(
    post_mu_qval     = map2_dbl(customer_mu,     posterior_mu,     ~ ecdf(.y)(.x)),
    post_lambda_qval = map2_dbl(customer_lambda, posterior_lambda, ~ ecdf(.y)(.x))
    )

customer_postcheck_tbl %>% glimpse()
```


```{r plot_posterior_params_facets, echo=TRUE}
postcheck_facet_plotdata_tbl <- customer_basic_pnbd_posterior_tbl %>%
  ungroup() %>%
  group_nest(customer_id) %>%
  slice_sample(n = 20) %>%
  unnest(data) %>%
  inner_join(customer_basic_parameters_tbl, by = "customer_id")

ggplot(postcheck_facet_plotdata_tbl) +
  geom_histogram(aes(x = mu), bins = 50) +
  geom_vline(aes(xintercept = customer_mu), colour = "red") +
  facet_wrap(facets = "customer_id", scales = "free")
```


## Construct Second Stan Model

We now want to improve the model by adding priors around the hyper-parameters
for how the $\mu$ and $\lambda$ parameters are distributed. To do this, we
try using Gamma distributed hyper-priors.

```{r compile_basic_prior_model, echo=TRUE}
pnbd_basic_prior_stanmodel <- stan_model(
  "stan_code/clv_pnbd_prior.stan",
  model_name    = "pnbd_basic_prior_model",
  warn_pedantic =  TRUE,
  verbose       = FALSE
  )
```


```{r fit_basic_prior_stan_model, echo=TRUE, warning=TRUE}
stan_data_lst <- customer_basic_pnbd_cbs_lst

pnbd_basic_prior_stanfit <- sampling(
  pnbd_basic_prior_stanmodel,
  data    = stan_data_lst,
  chains  =    4,
  iter    = 2000,
  # control = list(
  #   adapt_delta   = 0.995,
  #   max_treedepth = 15
  #   ),
  seed    = 4242
  )

pnbd_basic_prior_stanfit %>% summary()
```


We first want to check the HMC diagnostics.

```{r calculate_basic_prior_hmc_diagnostics, echo=TRUE}
pnbd_basic_prior_stanfit %>% check_hmc_diagnostics()

pnbd_basic_prior_stanfit %>% get_bfmi() %>% round(4)
```





# R Environment

```{r show_session_info, echo=TRUE, message=TRUE}
options(width = 120L)
sessioninfo::session_info()
options(width = 80L)
```
