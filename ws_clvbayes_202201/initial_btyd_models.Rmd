---
title: "Using Buy-Till-You-Die (BTYD) Models the Online Retail Dataset"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Last updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  rmdformats::readthedown:
    toc_depth: 3
    use_bookdown: yes
    css: styles.css

  html_document:
    fig_caption: yes
    theme: spacelab #sandstone #spacelab #flatly
    highlight: pygments
    number_sections: TRUE
    toc: TRUE
    toc_depth: 2
    toc_float:
      smooth_scroll: FALSE

  pdf_document: default
---


```{r import_libraries, echo=FALSE, message=FALSE}
library(conflicted)
library(tidyverse)
library(scales)
library(cowplot)
library(directlabels)
library(magrittr)
library(rlang)
library(fs)
library(purrr)
library(furrr)
library(glue)
library(CLVTools)
library(tidyquant)
library(rsample)
library(MASS)
library(fitdistrplus)
library(rstan)
library(brms)
library(posterior)
library(bayesplot)
library(tidybayes)


source("lib_utils.R")

conflict_lst <- resolve_conflicts(
  c("magrittr", "rlang", "dplyr", "readr", "purrr", "ggplot2", "MASS",
    "fitdistrplus")
  )


knitr::opts_chunk$set(
  tidy       = FALSE,
  cache      = FALSE,
  warning    = FALSE,
  message    = FALSE,
  fig.height =     8,
  fig.width  =    11
  )

options(
  width = 80L,
  warn  = 1,
  mc.cores = parallel::detectCores()
  )

theme_set(theme_cowplot())

set.seed(42)

plan(multisession)
```


# Load Data

We first want to load our datasets and perform some basic manipulation similar
to those done for the association rules mining.

```{r load_transaction_data, echo=TRUE}
tnx_data_tbl <- read_rds("data/retail_data_cleaned_tbl.rds")

tnx_data_tbl %>% glimpse()
```


We model the purchase data first, then combine this to create an individual
customer/invoice pairing with the total amount spent as an additional column.

```{r prepare_purchase_data, echo=TRUE}
tnx_purchase_tbl <- tnx_data_tbl %>%
  filter(
    quantity > 0,
    price > 0,
    exclude == FALSE
    ) %>%
  drop_na(customer_id) %>%
  select(
    invoice_date, invoice_id, stock_code, customer_id, description,
    quantity, price, stock_value
    )

tnx_purchase_tbl %>% glimpse()
```

Use of BTYD models assumes a total spend over a period of day and those
differences between the times. This is calculated internally by the various
BTYD routines so we keep just the per-invoice spend.

```{r calculate_customer_daily_spend, echo=TRUE}
daily_spend_invoice_tbl <- tnx_purchase_tbl %>%
  drop_na(customer_id) %>%
  group_by(invoice_date, customer_id, invoice_id) %>%
  summarise(
    .groups = "drop",
    
    invoice_spend = sum(stock_value)
    )

daily_spend_invoice_tbl %>% glimpse()


daily_spend_tbl <- daily_spend_invoice_tbl %>%
  group_by(invoice_date, customer_id) %>%
  summarise(
    .groups = "drop",
    
    total_spend = sum(invoice_spend),
    tnx_count   = n()
    )

daily_spend_tbl %>% glimpse()
```

Much of this analysis will require the cohort data for each of the customers
so we need to load them now.

```{r load_customer_cohort_data, echo=TRUE}
customer_cohort_tbl <- read_rds("data/customer_cohort_tbl.rds")
```



Finally, we want to set a date to make the end of the "fitting" data, which
will be used in the various estimation routines to fit parameters of our models.

```{r set_training_data_date, echo=TRUE}
training_data_date <- as.Date("2011-03-31")
```


# Background Theory

Before we start working on fitting and using the various Buy-Till-You-Die
models, we first need to discuss the basic underlying theory and model.

In this model, we assume a customer becomes 'alive' to the business at the
first purchase and then makes purchases stochastically but at a steady-rate
for a period of time, and then 'dies' - i.e. becomes inactive to the business -
hence the use of "Buy-Till-You-Die".

Thus, at a high level these models decompose into modelling the transaction
events using distributions such as the Poisson or Negative Binomial, and then
modelling the 'dropout' process using some other method.

A number of BTYD models exist and for this workshop we will focus on the
BG/NBD model - the Beta-Geometric Negative Binomial Distribution model (though
we will discuss the P/NBD model also).

These models require only two pieces of information about each customer's
purchasing history: the "recency" (when the last transaction occurred) and
"frequency" (the count of transactions made by that customer in a specified
time period).

The notation used to represent this information is

$$
X = (x, t_x, T),
$$
where $x$ is the number of transactions observed in the time period $T$ and
$t_x$ was the time of the last transaction.

From this summary data we can fit both P/NBD and BG/NBD models.


## Beta-Geometric/Negative-Binomial Distribution (BG/NBD) Model

This model relies on a number of base assumptions:

  1. While active, the number of transactions made by a customer follows a
  Poisson process with transaction rate $\lambda$.
  1. Heterogeneity in $\lambda$ follows a Gamma distribution
  $\Gamma(\lambda \, | \, \alpha, r)$ with parameters shape $r$ and rate
  $\alpha$. 
  1. After any transaction, a customer becomes inactive with probability $p$.
  1. Heterogeneity in $p$ follows a Beta distribution $B(p \, | \, a, b)$ with
  shape parameters $a$ and $b$.
  1. The transaction rate $\lambda$ and the dropout probability $p$ vary
  independently across customers.


Note that it follows from the above assumptions that the probability of a
customer being 'alive' after any transaction is given by the Geometric
distribution, and hence the Beta-Geometric in the name.


## Pareto/Negative-Binomial Distribution (P/NBD) Model

Similar to the BG/NBD model, the P/NBD model relies on five assumptions:

  1. While active, the number of transactions made by a customer follows a
  Poisson process with transaction rate $\lambda$.
  1. Heterogeneity in $\lambda$ follows a Gamma distribution
  $\Gamma(\lambda \, | \, \alpha, r)$ with shape $r$ and rate $\alpha$. 
  1. Each customer has an unobserved 'lifetime' of length $\tau$. This point at
  which the customer becomes inactive is distributed as an exponential with
  dropout rate $\mu$.
  1. Heterogeneity in dropout rates across customers follows a Gamma
  distribution $\Gamma(\mu \, | \, s, \beta)$ with shape parameter $s$ and
  rate parameter $\beta$.
  1. The transaction rate $\lambda$ and the dropout rate $\mu$ vary
  independently across customers.


## Gamma/Gamma Spending Model

The final piece of this approach is to construct a spending model for the
transactions. Similar to transaction rates, spending patterns are often
skewed, so a Gamma distribution is used.

For the Gamma/Gamma model, we use the following assumptions:

  1. Each transaction amount, $v_i$, is distributed according to a Gamma
  distribution $\Gamma(v_i \, | \, p, \nu)$ with shape parameter $p$ and
  rate parameter $\nu$.
  2. Heterogeneity in $p$ follows a Gamma distribution
  $\Gamma(p \, | \, q, \gamma)$




# Initial Models

Before we fit a full P/NBD Model we first wish to focus on more simple models:
we fit the counting process and the lifetime process separately - making sure
both of these fit appropriately and then focus on constructing the combined
model.

While this approach may seem pedantic and tedious, in practice we often
experience issues fitting the data and so it is better to build simpler models





# Construct Stan Models

## Data Preparation

We now need to prepare the data in the formats required to fit these models.

For the purposes of the P/NBD model, we only require three summary statistics
for each customer. In each case, the units of time are calculated relative to
when the customer appears on the system - this allows us to account for the
fact that a customer *born* more recently has had less time to make purchases.

Thus, each customers transaction data is summaries by a triplet of three values
$(x, t_x, t_{\text{cal}})$


$$
\begin{eqnarray}
x              &=& \text{ count of transactions in the data}, \\
t_x            &=& \text{ time of the most recently made transaction}, \\
t_{\text{cal}} &=& \text{ time since first transaction made by the customer}
\end{eqnarray}
$$

```{r construct_clvdata, echo=TRUE}
customer_clvdata_tbl <- daily_spend_tbl %>%
  left_join(customer_cohort_tbl, by = "customer_id") %>%
  filter(cohort_qtr == "2009 Q4") %>%
  clvdata(
    date.format = "%Y-%m-%d",
    time.unit = "weeks",
    estimation.split = training_data_date,
    name.id          = "customer_id",
    name.date        = "invoice_date",
    name.price       = "total_spend"
    )

customer_clvdata_tbl %>% glimpse()
```

Now that we have constructed the `clvdata` structure, we can use this input
to construct the summary statistics required for our models. This data is
performed by internal routines in `CLVTools`, so we use those directly.

```{r construct_pnbd_input_data, echo=TRUE}
customer_pnbd_cbs_tbl <- customer_clvdata_tbl %>%
  (CLVTools:::pnbd_cbs)()

pnbd_data_lst <- list(
  N         = customer_pnbd_cbs_tbl %>% nrow(),
  n         = customer_pnbd_cbs_tbl %>% nrow(),
  
  T         = customer_pnbd_cbs_tbl %>% pull(T.cal),

  recency   = customer_pnbd_cbs_tbl %>% pull(t.x),
  t         = customer_pnbd_cbs_tbl %>% pull(t.x),
  
  frequency = customer_pnbd_cbs_tbl %>% pull(x),
  k         = customer_pnbd_cbs_tbl %>% pull(x)
  )

pnbd_data_lst %>% glimpse()
```



## Construct Fixed Priors Model

```{r construct_fixed_stan_model, echo=TRUE}
pnbd_fixed_stanmodel <- stan_model(
  "stan_code/clv_pnbd_fixed.stan",
  model_name = "pnbd_fixed_model",
  verbose    = TRUE
  )
```


```{r fit_fixed_stan_model, echo=TRUE, cache=TRUE}
stan_data_lst <- compose_data(
  pnbd_data_lst,
  
  etau_shape   =  1,
  etau_rate    = 10,
  
  lambda_shape =  1,
  lambda_rate  = 10
  )


pnbd_fixed_stanfit <- sampling(
  pnbd_fixed_stanmodel,
  data   = stan_data_lst,
  chains = 4,
  iter   = 1000,
  seed   = 42
  )

pnbd_fixed_stanfit %>% summary()
```


```{r extract_fixed_stanmodel_params, echo=TRUE}
pnbd_fixed_postparams_tbl <- pnbd_fixed_stanfit %>%
  tidy_draws() %>%
  pivot_longer(
    !c(.chain, .iteration, .draw),
    names_to  = "parameter",
    values_to = "value"
    )

pnbd_fixed_postparams_tbl %>% glimpse()
```


## Construct Basic Priors Model

```{r construct_priors_stan_model, echo=TRUE}
pnbd_priors_stanmodel <- stan_model(
  "stan_code/clv_pnbd_priors.stan",
  model_name = "pnbd_priors_model",
  verbose    = TRUE
  )
```


```{r fit_priors_stan_model, echo=TRUE}
stan_data_lst <- compose_data(
  pnbd_data_lst,
  
  lambda_rate = 1,
  etau_rate   = 1
  )


pnbd_priors_stanfit <- sampling(
  pnbd_priors_stanmodel,
  data   = stan_data_lst,
  chains = 4,
  iter   = 1000,
  seed   = 42
  )

pnbd_priors_stanfit %>% summary()
```


```{r plot_priors_stanmodel_params, echo=TRUE}
# pnbd_priors_postparams_tbl <- pnbd_priors_stanfit %>%
#   tidy_draws() %>%
#   pivot_longer(
#     !c(.chain, .iteration, .draw),
#     names_to  = "parameter",
#     values_to = "value"
#     )
# 
# pnbd_priors_postparams_tbl %>% glimpse()
```




# R Environment

```{r show_session_info, echo=TRUE, message=TRUE}
sessioninfo::session_info()
```
