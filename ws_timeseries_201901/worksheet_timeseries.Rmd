---
title: "Dublin Data Science Workshop on Time Series Analysis"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "Monday, 21 January 2019"
output:
  html_document:
    fig_caption: yes
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 3
    toc_float:
      smooth_scroll: FALSE
  pdf_document: default
---


```{r knit_opts, include = FALSE}
knitr::opts_chunk$set(tidy  = FALSE
                     ,cache = FALSE
                     ,message = FALSE
                     ,warning = FALSE
                     ,fig.height =  8
                     ,fig.width  = 11)

library(tidyverse)
library(tidyquant)
library(scales)
library(cowplot)
library(cranlogs)
library(timetk)
library(sweep)


options(width = 80L
       ,warn  = 1
        )

set.seed(42)

theme_set(theme_cowplot())


source('custom_functions.R')
```



# Introduction

Time series occur in most fields of study that produce quantitative data.
Whenever quantities are measured over time, those measurements form a
time-series, or more formally, a discrete-time stochastic process.

## Workshop Materials

All materials for this workshop is available in my standard GitHub repo:

https://github.com/kaybenleroll/dublin_r_workshops


![](img/itswr_cover.jpg)


The content of this workshop is partly based on the book "Introductory Time
Series with R" by Paul Cowpertwait and Andrew Metcalfe. The data from this book
is available from the website:

http://www.maths.adelaide.edu.au/emac2009/


### R Packages

In this workshop we focus on the use of 'tidy'-style tools in the analysis of
time-series. In particular we look at packages such as `tidyquant` that enable
and simplify this approach to time-series analysis.


## Basic Concepts

A famous example of a time-series is count of airline passengers in the US,
as shown in the figure below. This is a simple univariate time-series, with
measurements taken on a monthly basis over a number of years, each datum
consisting of a single number - the number of passengers travelling via
a commercial flight that month.

```{r plot_air_passenger_timeseries, echo=TRUE}
plot(AirPassengers)
```

Before we begin analysing such data, we first need to create a mathematical
framework to work in. Fortunately, we do not need anything too
complicated, and for a finite time-series of length $N$, we model the time
series as a sequence of $N$ random variables, $X_i$, with $i = 1, 2, ..., N$.

Note that each individual $X_i$ is a wholly separate random variable: we only
ever have a single measurement from each random variable. In many cases we
simplify this, but it is important to understand and appreciate that such
simplifications are just that. Time series are difficult to analyse.

Before we get to any of that though, and before we try to build any kind of
models for the data, we always start with visualising the data. Often, a simple
plot of the data helps use pick out aspects to analyse and incorporate into the
models. For time series, one of the first things to do is the *time plot*, a
simple plot of the data over time.

For the passenger data, a few aspects stand out that are very common in time
series. It is apparent that the numbers increase over time, and this systematic
change in the data is called the *trend*. Often, approximating the trend as a
linear function of time is adequate for many data sets.

A repeating pattern in the data that occurs over the period of the data (in
this case, each year), is called the *seasonal variation*, though a more
general concept of 'season' is implied --- it often will not coincide with the
seasons of the calendar.

A slightly more generalised concept from the seasonality is that of *cycles*,
repeating patterns in the data that do not correspond to the natural fixed
periods of the model. None of these are apparent in the air passenger data, and
accounting for them are beyond the scope of this introductory tutorial.

Finally, another important benefit of visualising the data is that it helps
identify possible *outliers* and *erroneous* data.

In many cases, we will also be dealing with time series that have multiple
values at all, many or some of the points in time.

Often, these values will be related in some ways, and we will want to analyse
those relationships also. In fact, one of the most efficient methods of
prediction is to find *leading indicators* for the value or values you wish to
predict --- you can often use the current values of the leading indicators to
make inference on future values of the related quantities.

The fact that this is one of the best methods in time series analysis says a
lot about the difficulty of prediction (Yogi Berra, a US baseball player noted
for his pithy statements, once said "Prediction is difficult, especially about
the future").


## Example Timeseries

In this workshop we will look at a number of different time-series, discussed
here.

This data comes in a few different format, and this workshop discusses methods
for analysing this data in a common format.


### Air Passenger Data

As mentioned previously, a canonical time-series is the airline passenger
dataset, and this is the first dataset we look at.

```{r show_airline_passengers}
AirPassengers %>% print()

AirPassengers %>% plot()
```

In this workshop we will convert all time series into the tibbles: the package
`timetk` allows us to do this.

```{r convert_airpassengers_tibble, echo=TRUE}
airpassengers_tbl <- AirPassengers %>% tk_tbl(rename_index = 'month')

airpassengers_tbl %>% print()


ggplot(airpassengers_tbl) +
    geom_line(aes(x = month, y = value)) +
    xlab('Date') +
    ylab('Passenger Count') +
    expand_limits(y = 0) +
    ggtitle('Plot of Air Passenger Time Series')
```


### Maine Unemployment Data

Time series are very common in econometrics, and a dataset provided in the
text is that of monthly unemployment statistics in Maine from 1996 on. I have
included the datafile in this workshop.

```{r load_maine_unemployment, echo=TRUE}
maine_ts <- scan('data/Maine.dat', skip = 1) %>%
    ts(start = 1996, frequency = 12)

maine_ts %>% print()
maine_ts %>% plot()
```

As before, we convert this data into a tibble and recreate the plot using
`ggplot2`.

```{r maine_unemployment_tibble, echo=TRUE}
maine_tbl <- maine_ts %>% tk_tbl(rename_index = 'month')

maine_tbl %>% print()


ggplot(maine_tbl) +
    geom_line(aes(x = month, y = value)) +
    xlab('Date') +
    ylab('Unemployment Numbers') +
    expand_limits(y = 0) +
    ggtitle('Plot of Maine Unemployment Time Series')
```


### Australian Consumption Statistics (CBE)

Governments produce regular data on consumption numbers for their economy. One
such dataset is contained in the file `cbe.dat`, containing data of chocolate,
beer and energy production on a monthly basis.

```{r load_australian_cbe_data, echo=TRUE}
cbe_raw_tbl <- read_tsv('data/cbe.dat')

cbe_raw_tbl %>% glimpse()
```

Similar to the Maine file, this data does not contain time indices for the
data. For the sake of completeness, we use the same approach as before and
convert to a tibble, but we will also show how to construct the time index
without having to do intermediate conversions.

First we add time indices via `ts` conversions.


```{r construct_cbe_timeseries, echo=TRUE}
cbe_ts <- cbe_raw_tbl %>%
    as.matrix() %>%
    ts(start = 1958, frequency = 12)

cbe_ts %>% head(10)
cbe_ts %>% plot()
```

An alternative approach is to add the time index directly.

```{r australian_cbe_add_time, echo=TRUE}
n_data <- cbe_raw_tbl %>% nrow()

cbe_tbl <- cbe_raw_tbl %>%
    add_column(month = (1958 + (0:(n_data-1)/12)) %>% yearmon, .before = 1)


cbe_tbl %>% glimpse()
cbe_tbl %>% print()
```

Having constructed the tibble, we now construct these time series plots using
`ggplot2`.

```{r construct_cbe_plots, echo=TRUE}
plot_tbl <- cbe_tbl %>%
    gather('product', 'value', -month)

ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value, colour = product)) +
    expand_limits(y = 0) +
    xlab("Date") +
    ylab("Production Amount") +
    scale_y_continuous(labels = comma) +
    ggtitle('Production Data from Australian Government')
```


Due to the different scales, it might be more useful to use a faceted plot for
each product:

```{r plot_cbe_facetted_data, echo=TRUE}
plot_tbl <- cbe_tbl %>%
    gather('product', 'value', -month)

ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value)) +
    facet_grid(rows = vars(product), scales = 'free_y') +
    expand_limits(y = 0) +
    xlab("Date") +
    ylab("Production Amount") +
    scale_y_continuous(labels = comma) +
    ggtitle('Production Data from Australian Government')
```


### CRAN Package Downloads Data

An interesting dataset is the daily count of package downloads from CRAN. This
data is easy to obtain via use of the package `cranlogs`, which gives us use
of the `cran_downloads()` function.

For this workshop, we will look at some of the main packages that comprise
the 'tidyverse', as well as the total number of downloads from CRAN.

```{r download_cran_data, echo=TRUE}
cran_data_file <- 'data/cran_download_data.csv'

if(file.exists(cran_data_file)) {
    cran_data_tbl <- read_csv(cran_data_file)
} else {
    cran_pkgs <- c('dplyr', 'tidyr', 'ggplot2', 'lubridate', 'stringr', 'tibble'
                  ,'broom', 'jsonlite', 'purrr', 'readr', 'tidyquant')
      

    cran_data_tbl <- retrieve_cran_download_data(cran_pkgs, '2014-01-01', '2018-12-31')
    
    write_csv(cran_data_tbl, path = cran_data_file)
}


cran_data_tbl %>% glimpse()
cran_data_tbl %>% print()
```

First we construct a simple line plot of the download counts, facetted by
package.

```{r plot_cran_downloads, echo=TRUE}
ggplot(cran_data_tbl) +
    geom_line(aes(x = date, y = count)) +
    facet_wrap(vars(package), scales = 'free_y') +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    ggtitle('Facetted Lineplots of CRAN Daily Downloads')
```

Not all packages have download data as some packages were created after the
start of our observation period. This manifests as zero counts for that
package. We discuss these issues later on in the workshop.


## Combining Time Series

Useful insights are often gained from combining different datasets together.

Looking at our datasets, one possible interesting relationship is that between
energy production and airline passengers -- it is reasonable to expect that
both of these quantities will be related as they are related to the overall
health and size of the economy.

A major benefit of using tidy tools for time series is to make such data
manipulation and arrangement simple: combining datasets is simply a matter
of using the two-table joins.

To illustrate, we combine the Air Passenger data with the Australian economic
data, using the following code. Note that we rename the air passenger data
at the end to make it more meaningful and useful.


```{r combining_airpassengers_australia_data, echo=TRUE}
ap_econ_combined_tbl <- airpassengers_tbl %>%
    left_join(cbe_tbl, by = 'month') %>%
    filter(complete.cases(.)) %>%
    rename(air = value)


ap_econ_combined_tbl %>% glimpse()
ap_econ_combined_tbl %>% print()
```

We return to this dataset later in the workshop.


# Manipulation of Time Series Data

Much like all data, it is rate to get time-series exactly in the format we
want for analysis. For various reasons, we may want to analyse transformations
or aggregations of this data.

Much like feature engineering and variable selection, this process can be
more art than science - there are no hard and fast rules, merely principles
and rules-of-thumb.

The last few years in particular have seen the development of a number of
tools to aid us with the analysis of time series along 'tidy' principles. In
particular, we will focus on the use of `tidyquant` - a package aimed at
analysing financial data, but which is also very useful for time series.


## Aggregating Data

From a conceptual point of view, aggregating time series is the most
straightforward - we group the data by longer periods of time and aggregate
each individual 'bucket' of data as desired or required.


### Single Statistics

As an example of this, suppose we wish to look at the air passenger data as an
annual sum for each year. Our data is monthly, so we need to aggregate this
data into annual numbers.

```{r aggregate_ap_data_annual, echo=TRUE}
ap_yearly_dplyr_tbl <- airpassengers_tbl %>%
    group_by(year = month %>% format('%Y') %>% as.numeric()) %>%
    summarise(ann_total = sum(value))


ap_yearly_dplyr_tbl %>% glimpse()
ap_yearly_dplyr_tbl %>% print()
```

The above transformation was straightforward using existing `dplyr`
functionality, but we can also use routines provided for by `tidyquant` and its
function `tq_transmute`:


```{r aggregate_ap_annual_transmute, echo=TRUE}
ap_yearly_tidyquant_tbl <- airpassengers_tbl %>%
    tq_transmute(
        select     = value
       ,mutate_fun = apply.yearly
       ,FUN        = sum
       ,na.rm      = TRUE
       ,col_rename = 'ann_total'
    ) %>%
    mutate(year = month %>% format('%Y') %>% as.numeric())


ap_yearly_tidyquant_tbl %>% glimpse()
ap_yearly_tidyquant_tbl %>% print()
```

We can now look at a lineplot for this new time-series of annual totals. This
gives us an idea of the overall trend in the data.


```{r aggregate_ap_data_annual_lineplot, echo=TRUE}
ggplot(ap_yearly_tidyquant_tbl) +
    geom_line(aes(x = year, y = ann_total)) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    ggtitle('Lineplot of the Annual Air Passenger Totals')
```

#### Exercises

  1. Aggregate the Maine unemployment data into yearly totals.
  1. Create the relevant lineplots to check this data for trends and patterns.
  1. Aggregate the CBE data into yearly totals.
  1. Create plots of this data to check for trends.


### Multiple Statistics

Should we need multiple statistics in our output, we implement this by
writing a custom function that outputs the statistics we require.

```{r aggregate_ap_annual_transmute_custom, echo=TRUE}
custom_stats_func <- function(x, na.rm = TRUE, ...) {
    c(
          sum    = sum(x, na.rm = na.rm)
         ,mean   = mean(x, na.rm = na.rm)
         ,sd     = sd(x, na.rm = na.rm)
         ,median = median(x, na.rm = na.rm)
         ,q25    = quantile(x, na.rm = na.rm, probs = 0.25, names = FALSE)
         ,q75    = quantile(x, na.rm = na.rm, probs = 0.75, names = FALSE)
    )
}


ap_yearly_tidyquant_custom_tbl <- airpassengers_tbl %>%
    tq_transmute(
        select     = value
       ,mutate_fun = apply.yearly
       ,FUN        = custom_stats_func
       ,na.rm      = TRUE
    ) %>%
    mutate(year = month %>% format('%Y') %>% as.numeric())


ap_yearly_tidyquant_custom_tbl %>% glimpse()
ap_yearly_tidyquant_custom_tbl %>% print()
```


#### Exercises

  1. Repeat the aggregation shown using the Maine unemployment data
  1. Repeat the aggregation shown using the CBE data.
  1. Create a custom function to calculate mean, sd, skew and kurtosis


## Rolling Functions

Another common transformation of time-series is to apply a function over a
fixed rolling window of data.

Note that rolling functions different conceptually from aggregates as they
are not calculated over disjoint subsets of the data: the output is at the
same time period as the original data.

Because of this difference we use a different function from `tidyquant` to
execute this calculation: `tq_mutate()`:


### Moving Averages

A common rolling function is the *moving average*: we calculate the average
value of the time series over a fixed window of data.

```{r rolling_ap_sixmonth_mutate, echo=TRUE}
ap_rollmean_sixmonth_tbl <- airpassengers_tbl %>%
    tq_mutate(
        # tq_mutate args
        select     = value
       ,mutate_fun = rollapply 
        # rollapply args
       ,width      = 6
       ,align      = "right"
       ,FUN        = mean
        # mean args
       ,na.rm      = TRUE
        # tq_mutate args
       ,col_rename = "mean_6m"
    )


ap_rollmean_sixmonth_tbl %>% glimpse()
ap_rollmean_sixmonth_tbl %>% print()
```

We compare the two values by plotting the original time series against its
moving average.


```{r rolling_ap_sixmonth_plot, echo=TRUE}
plot_tbl <- ap_rollmean_sixmonth_tbl %>%
    rename(orig = value) %>%
    gather('label', 'value', -month)


ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value, colour = label)) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    xlab('Month') +
    ylab('Passenger Total') +
    ggtitle('Comparison Plot of the Air Passenger Counts')
```

Note that the moving-average series does not start at the same timestamp as
the original dataset size is reduced by the windowing function.

We can add multiple moving averages to a time series by chaining a series of
`tq_mutate()` calls together.


```{r rolling_ap_multiwindow, echo=TRUE}
ap_rollmean_multi_tbl <- airpassengers_tbl %>%
    tq_mutate(
        # tq_mutate args
        select     = value
       ,mutate_fun = rollapply
        # rollapply args
       ,width      = 6
       ,align      = "right"
       ,FUN        = mean
        # mean args
       ,na.rm      = TRUE
        # tq_mutate args
       ,col_rename = "mean_6m"
    ) %>%
    tq_mutate(
        # tq_mutate args
        select     = value
       ,mutate_fun = rollapply 
        # rollapply args
       ,width      = 12
       ,align      = "right"
       ,FUN        = mean
        # mean args
       ,na.rm      = TRUE
        # tq_mutate args
       ,col_rename = "mean_12m"
    )
  

ap_rollmean_multi_tbl %>% glimpse()
ap_rollmean_multi_tbl %>% print()
```

As before, we now create a lineplot of the three values to show the effect of
the different window sizes.

```{r rolling_ap_multi_plot, echo=TRUE}
plot_tbl <- ap_rollmean_multi_tbl %>%
    rename(orig = value) %>%
    gather('label', 'value', -month)


ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value, colour = label)) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    xlab('Month') +
    ylab('Passenger Total') +
    ggtitle('Comparison Plot of the Air Passenger Counts')
```

The twelve month time series is shorter than the six month series as it has
a wider calculation window.

Any sort of other windowing functions can be applied, including the standard
deviation, allowing us to include a range of possible values.


```{r rolling_ap_ribbon, echo=TRUE}
ribbon_func <- function(x, na.rm = TRUE, ...) {
    mu    <- mean(x, na.rm = na.rm)
    sigma <- sd(x, na.rm = na.rm)
    
    lower <- mu - 2 * sigma
    upper <- mu + 2 * sigma
    
    return(c(mu = mu, l2sd = lower, u2sd = upper))
}


ap_roll_ribbon_tbl <- airpassengers_tbl %>%
    tq_mutate(
        # tq_mutate args
        select     = value
       ,mutate_fun = rollapply 
        # rollapply args
       ,width      = 6
       ,align      = "right"
       ,by.column  = FALSE
       ,FUN        = ribbon_func
        # mean args
       ,na.rm      = TRUE
    )
  

ap_roll_ribbon_tbl %>% glimpse()
ap_roll_ribbon_tbl %>% print()
```

We now plot the original data against the moving average and the mean.


```{r plot_ap_6m_ribbon_data, echo=TRUE}
ggplot(ap_roll_ribbon_tbl) +
    geom_line(aes(x = month, y = value)) +
    geom_line(aes(x = month, y = mu), colour = 'red') +
    geom_ribbon(aes(x = month, ymin = l2sd, ymax = u2sd)
               ,colour = 'grey', alpha = 0.25) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    xlab('Month') +
    ylab('Passenger Total') +
    ggtitle('Ribbon Plot of the Air Passenger Counts (6 month window)')
```


We now repeat this process with using a twelve-month window for the data.

```{r rolling_ap_12m_ribbon, echo=TRUE}
ap_roll_12m_ribbon_tbl <- airpassengers_tbl %>%
    tq_mutate(
        # tq_mutate args
        select     = value
       ,mutate_fun = rollapply 
        # rollapply args
       ,width      = 12
       ,align      = "right"
       ,by.column  = FALSE
       ,FUN        = ribbon_func
        # mean args
       ,na.rm      = TRUE
    )
  

ap_roll_12m_ribbon_tbl %>% glimpse()
ap_roll_12m_ribbon_tbl %>% print()
```

Having constructed the data, we once again create a ribbon plot with these
quantities.

```{r plot_ap_12m_ribbon_data, echo=TRUE}
ggplot(ap_roll_12m_ribbon_tbl) +
    geom_line(aes(x = month, y = value)) +
    geom_line(aes(x = month, y = mu), colour = 'red') +
    geom_ribbon(aes(x = month, ymin = l2sd, ymax = u2sd)
               ,colour = 'grey', alpha = 0.25) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    xlab('Month') +
    ylab('Passenger Total') +
    ggtitle('Ribbon Plot of the Air Passenger Counts (12 month window)')
```


#### Exercises

  1. Construct a 3 month moving average for the passenger data and compare it
     to the 6 and 12 month values.
  1. Calculate the 6 month and 12 month rolling average values for the Maine
     unemployment data.
  1. Construct the ribbon plot for the Maine unemployment data.
  1. Construct moving average data for the CBE dataset. This process may be
     made easier by reshaping the data.


### Differences

Another common transformation of the data is to take the 'first differences'
of the values, i.e. we convert the time series of values into one of
differences. We discuss the reasons for this later on -- for now we focus on
the mechanics of creating first differences.


```{r ap_firstdiff_mutate, echo=TRUE}
ap_firstdiff_tbl <- airpassengers_tbl %>%
    mutate(diff = value - lag(value, n = 1))


ap_firstdiff_tbl %>% glimpse()
ap_firstdiff_tbl %>% print()
```

Having calculated the differences, we now produce a lineplot of those values.

```{r ap_firstdiff_plot, echo=TRUE}
plot_tbl <- ap_firstdiff_tbl %>%
    rename(count = value) %>%
    gather('series', 'value', -month)


ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value, colour = series)) +
    scale_y_continuous(labels = comma) +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Value') +
    ggtitle('Plot of the Air Passenger Counts and First Differences')
```

As we see with this plot, the first differences of the passenger data does not
contain a trend.


#### Exercises

  1. Calculate the first differences for the Maine unemployment data.
  1. Create a lineplot of this data to check for its value.
  1. Calculate the first differences for the CBE data.
  1. Create lineplots for the CBE differences.
  1. Using the `lag()` function with the Air Passenger data, calculate the
     percentage changes data instead of the arithmetic changes.
  1. Construct the lineplot for the percentage change values.
  
  

# Exploratory Data Analysis of Time Series

The first step in all exploratory analysis is simple visualisation: simple
lines plots such as those we have seen are our starting point. The human
brain is excellent at pattern recognition, so a simple plot often guides our
analysis more effectively than a suite of numerical diagnostics.

Rather than discuss different techniques, we will explore our sample data as a
way to illustrate some ways of initially exploring the datasets.


## Air Passenger

We start with the air passenger data, and remind ourselves of the basic
structure of the data.

```{r airpassengers_explore_struct, echo=TRUE}
airpassengers_tbl %>% glimpse()
airpassengers_tbl %>% print()
```


### Raw Data

```{r plot_airpassenger_exploration, echo=TRUE}
ggplot(airpassengers_tbl) +
    geom_line(aes(x = month, y = value)) +
    xlab('Date') +
    ylab('Passenger Count') +
    expand_limits(y = 0) +
    ggtitle('Plot of Air Passenger Time Series')
```

This plot suggests a strong seasonal effect as well as a trend so this is the
first thing to investigate.

To look into trends, we have a number of options: we could look at yearly
sums or averages, or we could look at moving averages.

```{r plot_ap_yearly_totals, echo=TRUE}
ggplot(ap_yearly_tidyquant_tbl) +
    geom_line(aes(x = year, y = ann_total)) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    ggtitle('Lineplot of the Annual Air Passenger Totals')
```


One way to investigate the seasonality in the dataset is to construct a boxplot
of the passenger counts, grouping by month.

```{r plot_ap_monthly_boxplots, echo=TRUE}
plot_tbl <- airpassengers_tbl %>%
    mutate(cal_month = format(month, '%b'))

ggplot(plot_tbl) +
    geom_boxplot(aes(x = cal_month, y = value)) +
    scale_y_continuous(labels = comma) +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Passenger Count') +
    ggtitle('Boxplot of the Air Passenger Counts')
```

We can see some aspects of the data seasonality in this boxplot, but the
multiplicative nature of the plots means the seasonal trends is obscured a
little.

### First Differences

We also produce a similar boxplot, but this time looking at the first
differences.

```{r plot_ap_diffs_monthly_boxplots, echo=TRUE}
plot_tbl <- ap_firstdiff_tbl %>%
    mutate(cal_month = format(month, '%m'))

ggplot(plot_tbl) +
    geom_boxplot(aes(x = cal_month, y = diff)) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    xlab('Month') +
    ylab('Passenger Count Changes') +
    ggtitle('Boxplot of the First Differences of the Air Passenger Counts')
```

The seasonality in the data comes through much stronger with this plot. We see
much bigger monthly effects.


### Percentage Changes

To look into multiplicative effects we check the percentage changes from month
to month.


```{r ap_perc_change_explore_struct, echo=TRUE}
ap_perc_change_tbl <- airpassengers_tbl %>%
    mutate(cal_month   = format(month, '%m')
          ,perc_change = value / lag(value, n = 1) - 1)


ap_perc_change_tbl %>% glimpse()
ap_perc_change_tbl %>% print()
```

Having looked at the data and the column, we now look at some simple lineplots
as before.

```{r plot_ap_perc_change_lineplot, echo=TRUE}
ggplot(ap_perc_change_tbl) +
    geom_line(aes(x = month, y = perc_change)) +
    expand_limits(y = 0) +
    xlab('Date') +
    ylab('Percentage Change') +
    ggtitle('Lineplot of the Percentage Changes of the Air Passenger Counts')
```

Much like the arithmetic differences, the percentage changes are centred around
zero, so now we can look at a boxplot of them.

```{r plot_ap_perc_changes_monthly_boxplots, echo=TRUE}
ggplot(ap_perc_change_tbl) +
    geom_boxplot(aes(x = cal_month, y = perc_change)) +
    xlab('Month') +
    ylab('Percentage Changes') +
    ggtitle('Boxplot of the Percentage Changes of the Air Passenger Counts')
```


## Maine Unemployment

We now explore the Maine unemployment data.

As before, we look at the raw data, the first differences and the percentage
changes.

```{r maine_data_exploration, echo=TRUE}
maine_explore_tbl <- maine_tbl %>%
    mutate(cal_month   = format(month, '%m')
          ,diff        = value - lag(value, n = 1)
          ,perc_change = value / lag(value, n = 1) - 1
           )


maine_explore_tbl %>% glimpse()
maine_explore_tbl %>% print()
```


### Raw Data

We start by the standard lineplot of the values.

```{r plot_maine_lineplot, echo=TRUE}
ggplot(maine_explore_tbl) +
    geom_line(aes(x = month, y = value)) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    xlab('Time') +
    ylab('Maine Unemployment Count (thousands)') +
    ggtitle('Lineplot of the Maine Unemployment Data')
```

We do not see a trend in this data but the data does seem to have strong
seasonal patterns.

To investigate the seasonality we construct the monthly boxplots from the raw
data. Employment is often seasonal in nature - for example, retail is very
busy towards Christmas each year. As such, we expect to see a large seasonal
component in this data.

```{r plot_maine_monthly_boxplot, echo=TRUE}
ggplot(maine_explore_tbl) +
    geom_boxplot(aes(x = cal_month, y = value)) +
    expand_limits(y = 0) +
    scale_y_continuous(labels = comma) +
    xlab('Month') +
    ylab('Maine Unemployment Count (thousands)') +
    ggtitle('Monthly Boxplot of the Maine Unemployment Data')
```

We see definite changes over the months, though the effect does not seem as
pronounced here as it was for the airline passenger counts.

### First Differences

We now look at first differences for the unemployment data, and start with the
line plot.

```{r plot_maine_diff_lineplot, echo=TRUE}
ggplot(maine_explore_tbl) +
    geom_line(aes(x = month, y = diff)) +
    expand_limits(y = 0) +
    xlab('Time') +
    ylab('Maine Unemployment Count Differences') +
    ggtitle('Lineplot of the Differences in Maine Unemployment Data')
```

The original data does not exhibit a strong trend, and the first differences
are similar.

```{r plot_maine_diff_monthly_boxplot, echo=TRUE}
ggplot(maine_explore_tbl) +
    geom_boxplot(aes(x = cal_month, y = diff)) +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Maine Unemployment Count Differences') +
    ggtitle('Monthly Boxplot of the Maine Unemployment Data')
```

We see strong seasonal differences in the monthly data.


### Percentage Changes

We look at the lineplot of the percentage changes next.

```{r plot_maine_perc_change_lineplot, echo=TRUE}
ggplot(maine_explore_tbl) +
    geom_line(aes(x = month, y = perc_change)) +
    expand_limits(y = 0) +
    xlab('Time') +
    ylab('Maine Unemployment Count Percentage Changes') +
    ggtitle('Lineplot of the Percentage Changes in Maine Unemployment Data')
```

As for the differences, no major trends or patterns emerge from this plot.

We now look for seasonal patterns in the percentage changes.

```{r plot_maine_perc_change_monthly_boxplot, echo=TRUE}
ggplot(maine_explore_tbl) +
    geom_boxplot(aes(x = cal_month, y = perc_change)) +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Maine Unemployment Count Percentage Changes') +
    ggtitle('Monthly Boxplot of the Percentage Changes in Maine Unemployment Data')
```



# Time Series Decomposition

Many time series are dominated by trends or seasonal effects, and we can
create fairly simple models based on these two components. The first of these,
the *additive decompositional model*, is just the sum of these effects, with
the residual component being treated as random:

$$
x_t = m_t + s_t + z_t,
$$

where, at any given time $t$,

\begin{eqnarray*}
x_t && \text{is the observed value} \\
m_t && \text{is the trend} \\
s_t && \text{is the seasonal component} \\
z_t && \text{is the error term}
\end{eqnarray*}


It is worth noting that, in general, the error terms will be a correlated
sequence of values, something we will account for and model later.

In other cases, we could have a situation where the seasonal effect
increases as the trend increases, modeling the values as:

$$
x_t = m_t s_t + z_t.
$$

Other options also exist, such as modeling the log of the observed values,
which does cause some non-trivial modeling issues, such as biasing any
predicted values for the time series.

Various methods are used for estimating the trend, such as taking a
*moving average* of the values, which is a common approach.


## Simple Decomposition

We start with the simplest decomposition: the simple additive model with a
moving average trend.

### Additive Models

The `decompose` function in R allows us to build this:

```{r decompose_ap_simple_additive, echo=TRUE}
ap_ts_sa_decompose <- airpassengers_tbl %>%
    tk_ts(select = value, start = 1949, frequency = 12) %>%
    decompose()

ap_ts_sa_decompose %>% plot()
```

The package `sweep` provides us with a number of routines to help us tidy the
output of these routines.

```{r decompose_ap_simple_additive_tidy, echo=TRUE}
ap_ts_sa_decompose_tbl <- ap_ts_sa_decompose %>%
    sw_tidy_decomp(rename_index = 'month')


ap_ts_sa_decompose_tbl %>% glimpse()
ap_ts_sa_decompose_tbl %>% print()
```

We have decomposed the time series into components, and the `sw_tidy_decomp()`
function calculates the 'seasonally adjusted' values of the trend i.e. the
'underlying' value ignoring seasonality.

Comparing the observed and seasonally adjusted values are straight-forward
from this, we plot both together.

```{r plot_passenger_sa_adjusted, echo=TRUE}
ggplot(ap_ts_sa_decompose_tbl) +
    geom_line(aes(x = month, y = observed)) +
    geom_line(aes(x = month, y = seasadj), colour = 'red') +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Passenger Count') +
    ggtitle('Comparison Lineplot of Passenger Data and Seasonal Counts'
           ,subtitle = '(seasonal adjustments in red)')

```


We now want to plot this data, and the easiest way to do this is to convert
it to 'long' format and plot each quantity separately.

```{r decompose_ap_sa_tidy_plot, echo=TRUE}
plot_tbl <- ap_ts_sa_decompose_tbl %>%
    gather('label', 'value', -month)

ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value)) +
    facet_grid(rows = vars(label)) +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Value') +
    ggtitle('Decomposition Plot of the Simple Additive Model')
```

Due to the difference in scales for each parameter, we redo this plot but we
free up the $y$-axis scale.


```{r decompose_ap_sa_tidy_scales_plot, echo=TRUE}
ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value)) +
    facet_grid(rows = vars(label), scales = 'free_y') +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Value') +
    ggtitle('Decomposition Plot of the Simple Additive Model')
```


#### Exercises

  1. Construct the simple additive decomposition on the Maine unemployment data.
  1. Construct the simple additive decomposition on the Australian CBE data.


### Multiplicative Models

Constructing the multiplicative decomposition model is similar, but this time
the seasonal component and error are multiplied to the trend rather than added.

Other than this, the multiplicative model works in a similar fashion. Once
again, while we focus on creating plots using `ggplot2` we will use the
default plotting system as a quick check.

```{r decompose_ap_simple_multiplicative, echo=TRUE}
ap_ts_sm_decompose <- airpassengers_tbl %>%
    tk_ts(select = value, start = 1949, frequency = 12) %>%
    decompose(type = 'multiplicative')

ap_ts_sm_decompose %>% plot()
```

Having created the decomposition object, we now inspect the outputs and produce
plots.

```{r decompose_ap_simple_multiplicative_tidy, echo=TRUE}
ap_ts_sm_decompose_tbl <- ap_ts_sm_decompose %>%
    sw_tidy_decomp(rename_index = 'month')

ap_ts_sm_decompose_tbl %>% glimpse()
ap_ts_sm_decompose_tbl %>% print()
```

While the model is different from before, it produces similar outputs, and so
our plot code is almost identical to before.

We compare the observed values to the seasonally-adjusted ones first via a
lineplot.

```{r plot_passenger_sm_adjusted, echo=TRUE}
ggplot(ap_ts_sm_decompose_tbl) +
    geom_line(aes(x = month, y = observed)) +
    geom_line(aes(x = month, y = seasadj), colour = 'red') +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Passenger Count') +
    ggtitle('Comparison Lineplot of Passenger Data and Seasonal Counts (Multiplicative Mdodel)'
           ,subtitle = '(seasonal adjustments in red)')

```

We then look at the full decomposition plot, and each of the components.

```{r decompose_ap_sm_tidy_plot, echo=TRUE}
plot_tbl <- ap_ts_sm_decompose_tbl %>%
    gather('label', 'value', -month)

ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value)) +
    facet_grid(rows = vars(label)) +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Value') +
    ggtitle('Decomposition Plot of the Simple Multiplicative Model for the Air Passenger Data')
```


Like for the additive model, the components of the decomposition are on
different scales, so we redo the plot with a free $y$-axis.

```{r decompose_ap_sm_tidy_scales_plot, echo=TRUE}
ggplot(plot_tbl) +
    geom_line(aes(x = month, y = value)) +
    facet_grid(rows = vars(label), scales = 'free_y') +
    expand_limits(y = 0) +
    xlab('Month') +
    ylab('Value') +
    ggtitle('Decomposition Plot of the Simple Multiplicative Model for the Air Passenger Data')
```



## STL Decomposition


# Autocorrelation


Assuming we can remove the trend and the seasonal variation, that
still leaves the random component, $z_t$. Unfortunately, analysing
this is usually highly non-trivial. As discussed, we model the random
component as a sequence of random variables, but no further
assumptions we made.

To simplify the analysis, we often make assumptions like
\emph{independent and identically distributed (i.i.d.)} random
variables, but this will rarely work well. Most of the time, the $z_t$
are correlated.

The \emph{expected value} or \emph{expectation} of a random variable
$x$, denoted $E(x)$, is the mean value of $x$ in the population. So,
for a continuous $x$, we have

$$
\mu = E(x) = \int p(x) \, x \, dx.
$$


and the \emph{variance}, $\sigma^2$, is the expectation of the squared
deviations,

$$
\sigma^2 = E[(x - \mu)^2],
$$

For bivariate data, each datapoint can be represented as $(x, y)$ and
we can generalise this concept to the \emph{covariance},
$\gamma(x, y)$,

$$
\gamma(x, y) = E[(x - \mu_x)(y - \mu_y)].
$$


Correlation, $\rho$, is the standardised covariance, dividing the
covariance by the standard deviation of the two variables,

$$
\rho(x, y) = \frac{\gamma(x, y)}{\sigma_x \sigma_y}.
$$


The mean function of a time series model is

$$
\mu(t) = E(x_t),
$$


with the expectation in this case being across the \emph{ensemble} of
possible time series that might have been produced by this model. Of
course, in many cases, we only have one realisation of the model, and
so, without any further assumption, estimate the mean to be the
measured value.

If the mean function is constant, we say that the time-series is
\emph{stationary in the mean}, and the estimate of the population mean
is just the sample mean,

$$
\mu = \sum^n_{t=1} x_t.
$$


The variance function of a time-series model that is stationary in the
mean is given by

$$
\sigma^2(t) = E[(x_t - \mu)^2],
$$


and if we make the further assumption that the time-series is also
stationary in the variance, then the population variance is just the
sample variance

$$
\text{Var}(x) = \frac{\sum(x_t - \mu)^2}{n - 1}
$$


Autocorrelation, often referred to as \emph{serial correlation}, is
the correlation between the random variables at different time
intervals. We can define the \emph{autocovariance function} and the
\emph{autocorrelation function} as functions of the \emph{lag}, $k$, as

\begin{eqnarray}
\gamma_k &=& E[(x_t - \mu)(x_{t+k} - \mu)], \\
\rho_k   &=& \frac{\gamma_k}{\sigma^2}.
\end{eqnarray}


Be default, the \texttt{acf()} function plots the \emph{correlogram},
which is a plot of the sample autocorrelation at $r_k$ against the lag
$k$.

\worksheetexercise
Using the function \texttt{acf()}, calculate the autocorrelations for
all the time series we have looked at. Look at the structure of the
output, and use the help system to see what options are provided.

\worksheetexercise
Check the output of \texttt{acf()} against manual calculations of the
correlations at various timesteps. Do the numbers match?

\noindent
\textbf{HINT:} The \texttt{cor()} function and some vector indexing
will be helpful here.

\worksheetexercise
Plot the output of the \texttt{acf()} for the different time
series. Think about what these plots are telling you. Do do these
plots help the modelling process, if so, how?

\worksheetexercise
Decompose the air passenger data and look at the appropriate
correlogram. What does this plot tell you? How does it differ from the
previous correlogram you looked at?

\worksheetexercise
How can we use all that we have learned so far to assess the efficacy
of the decompositional approach for time series?



# Basic Forecasting

As mentioned earlier, an efficient way to forecast a variable is to
find a related variable whose value leads it by one or more
timesteps. The closer the relationship and the longer the lead time,
the better it becomes.

The trick, of course, is to find a leading variable.

Multivariate series has a temporal equivalent to correlation and
covariance, known as the \emph{cross-covariance function (ccvf)} and
the \emph{cross-correlation function (ccf)},

\begin{eqnarray}
\gamma_k(x, y) &=& E[(x_{t+k} - \mu_x)(y_t - \mu_y)], \\
\rho_k(x, y)   &=& \frac{\gamma_k(x, y)}{\sigma_x \sigma_y}.
\end{eqnarray}


Note that the above functions are not symmetric, as the lag is always
on the first variable, $x$.


\worksheetexercise
Load the building approvals and activity data from the
\texttt{ApprovActiv.dat} file. The data is quarterly and starts in
1996. Determine which is the leading variable and investigate the
relationship between the two.

\worksheetexercise
Binding the time-series using \texttt{ts.union()}, find the
cross-correlations for the building data. Is the relationship
symmetric, and why?

\worksheetexercise
Examine the cross-correlations of the random element of the decomposed
time-series for the building data, and compare this to the original
cross-correlations.



Our main objective in forecasting is to estimate the value of a future
quantity, $x_{n+k}$, given past values ${x_1, x_2, ..., x_n}$. We
assume no seasonal or trend effects, or any such effects have been
removed from the data. We assume that the underlying mean of the data
is $\mu_t$, and that this value changes from timestep to timestep, but
this change is random.

Our model can be expressed as

$$
x_t = \mu_t + w_t,
$$

where $\mu_t$ is the non-stationary mean of the process at time $t$
and $w_t$ are independent random variates with mean $0$ and standard
deviation $\sigma$. We let $a_t$ be our estimate of $\mu_t$, and can
define the \emph{exponentially-weighted moving average (EWMA)}, $a_t$
to be

$$
a_t = \alpha x_t + (1 - \alpha) a_{t-1}, \;\;\; 0 \leq \alpha \leq 1.
$$


The value of $\alpha$ controls the amount of smoothing, as is referred
to as the \emph{smoothing parameter}.

\worksheetexercise
Load the data in the \texttt{motororg.dat} file. This is a count of
complaints received on a monthly basis by a motoring organisation from
1996 to 1999. Create an appropriate time series from this data. Plot
the data, checking it for trends or seasonality.

\worksheetexercise
Using the function \texttt{HoltWinters()}, with the additional
parameters set to zero, create the EWMA of the data, allowing the
function itself to choose the optimal value of $\alpha$. Investigate
and visualise the output, comparing it to the original time series.

\worksheetexercise
Specifying values of $\alpha$ of 0.2 and 0.9, create new versions of
the EWMA and compare them with previous fits of the EWMA.


The Holt-Winters method generalises this concept, allowing for trends
and seasonal effects. The equations that govern this model for
seasonal period, $p$, are given by

\begin{eqnarray}
a_t &=& \alpha (x_t - s_{t-p}) + (1 - \alpha)(a_{t-1} - b_{t-1}), \nonumber \\
b_t &=& \beta (a_t - a_{t-1}) + (1 - \beta)b_{t-1},\\
s_t &=& \gamma (x_t - a_t) + (1 - \gamma) s_{t-p}, \nonumber
\end{eqnarray}


where $a_t$, $b_t$, $s_t$ are the estimated level, slope and seasonal
effect at time $t$, and $\alpha$, $\beta$ and $\gamma$ are the
smoothing parameters.

\worksheetexercise
Fit the Holt-Winters parameters to the air passenger data and check
the fit. Visualise the raw time-series against the fitted data.

\worksheetexercise
Predict data ahead for four years and visualise this data. How
reliable are these forecasts do you think?



# Stochastic Methods and Regression

\noindent
A time series $w_t$ is \emph{discrete white noise} if the $w_t$ are
i.i.d with a mean of zero. Thus, they all have the same variance
$\sigma^2$ and $\text{Cor}(w_i, w_j) = 0$ for $i \neq j$. In addition,
if the $w_j \sim N(0, \sigma^2)$ then it is said to be
\emph{Gaussian white noise}.


A time series $x_t$ is a \emph{random walk} if

$$
x_t = x_{t-1} + w_t,
$$


where $w_t$ is a white-noise series.

\worksheetexercise
Generate a white noise series using \texttt{rnorm()}, with an initial
value, $w_0 = 1$. and length 100. Plot the output, and investigate its
correlogram.

\worksheetexercise
Generate a random walk time series with initial value $x_0 = 1$ and
length 100. Plot its output and investigate its correlogram.

\worksheetexercise
Think about how you might create a random walk with an underlying
drift?




The time series $x_t$ is a \emph{auto-regressive process of order $p$},
$\text{AR}(p)$, if,

\begin{equation}
x_t = \sum^p_{i=1} \alpha_i x_{t-i} + w_t,
\end{equation}

\noindent
where $w_t$ is a white-noise process and the $\alpha_p \neq 0$ for an
order-$p$ process.

\worksheetexercise
Generate data for an AR(1) model with $\alpha = 0.5$ and initial value
$x_1 = 1$. Plot the data and investigate its correlogram. Does this
time series appear to be stationary? The R function
\texttt{arima.sim()} can be used for this.

\worksheetexercise
Generate data for an AR(2) model with $\alpha_1 = 1$ and
$\alpha_2 = -0.25$ and initial value $x_1 = 1$, $x_2 = 1$. Plot the
data and investigate its correlogram. Does this time series appear to
be stationary?

\worksheetexercise
Generate data for an AR(2) model with $\alpha_1 = 0.5$ and
$\alpha_2 = 0.5$ and initial value $x_1 = 1$, $x_2 = 1$.

\worksheetexercise
Fit the time-series you generated above to an autoregressive model
using the function \texttt{ar()}. How do the fitted parameters match
the values you used?

\vspace{5mm}

\noindent
A moving average (MA) process of order $q$ is a linear combination of
the current white noise term and the $q$ most recent past white noise
terms,

\begin{equation}
x_t = w_t + \sum^q_{i=1} \beta_i w_{t - i}
\end{equation}

\noindent
where $w_t$ is a white-noise process with mean 0 and variance $\sigma^2$.

\worksheetexercise
Generate data for an MA(1) model with $\beta = 0.5$ and initial value
$x_1 = 1$. Plot the data and investigate its correlogram. Does this
time series appear to be stationary?

\worksheetexercise
Generate data for an MA(2) model with $\beta_1 = 1$ and
$\beta_2 = -0.25$ and initial values $x_1 = 1$, $x_2 = 2$. Plot the
data and investigate its correlogram. Does this time series appear to
be stationary?

\worksheetexercise
Generate data for an MA(2) model with $\beta_1 = 0.5$ and
$\beta_2 = 0.5$ and initial values $x_1 = 1$, $x_2 = 1$.

\worksheetexercise
Fit the time-series you generated above to a moving-average model
using the function \texttt{arima()}. How do the fitted parameters
match the values you used?

\worksheetexercise
Compare the AR and MA models to one another.



%%%
%%% SECTION: ARMA and ARIMA Models
%%%

\worksheetsection{ARMA and ARIMA Models}

\noindent
Now suppose we combine the ideas of autoregressive and moving average
models together. A time series follows an
\emph{autoregressive moving average (ARMA)} process of order $(p, q)$
when

\begin{equation}
x_t = \sum_{i=1}^p \alpha_i x_{t-i} + w_t + \sum_{j=1}^q \beta_j w_{t-j}
\end{equation}

\noindent
where $w_t$ is white noise.

Both $\text{AR}(p)$ and $\text{MA}(q)$ models are special cases of
$\text{ARMA}(p, q)$ (with $q = 0$ and $p = 0$ respectively), and ARMA
models are usually preferred due to \emph{parameter parsimony} ---
when fitting data, the ARMA model is usually more parameter efficient,
requiring few parameters.

\worksheetexercise
Using the R function \texttt{arima.sim()}, create an ARMA(1,1)
time-series of length 1000 with $\alpha = -0.6$, and
$\beta = 0.5$. Plot this time series and check its ACF. Is this
correct?

\worksheetexercise
Using \texttt{arima()}, fit your generated time series to an
$\text{ARMA}(1,1)$ model and compare the fitted output to the values
you have set.

\worksheetexercise
Repeat the above exercises, but for an $\text{ARMA}(2, 2)$ model with
$\alpha_1 = 0.2$, $\alpha_2 = -0.5$, $\beta_1 = -0.1$ and
$\beta_2 = 0.3$.

\worksheetexercise
Investigate the effect of the parameters $p$ and $q$ by generating the
various combinations of the $\text{ARMA}(p, q)$ models but using the
same set of innovations in each case.\\
\textbf{HINT:} \texttt{arima.sim()} has a parameter \texttt{innov = ...}
that allows you to pass in a set of innovations into the ARMA process.

\worksheetexercise
Load in the GBP/NZD currency pair data from the file
\texttt{pound\_nz.dat}, and create a time-series from this. The data is
quarterly, and starts in Q1 1991.

\worksheetexercise
Fit the data GBP/NZD to an $\text{MA}(1)$, an $\text{AR}(1)$ and an
$\text{ARMA}(1, 1)$ process. Which of the above models does the best
job at fitting the data?

\vspace{5mm}

\noindent
It may be becoming quickly apparent that the choice of parameter count
is non-trivial, and some kind of systematic approach would be
desirable.

One such method for choosing the optimal number of parameters is to
fit multiple options and choose the best one, using a metric known as
the \emph{Akaike Information Criterion (AIC)}. The AIC is defined to
be

\begin{equation}
\text{AIC} = -2 \times \text{loglikelihood of fit} + 2 \times
\text{parameter count},
\end{equation}

\noindent
so that it balances the better fitting of parameters while penalising
using too many parameters to fit.

\worksheetexercise
Use the R function \texttt{AIC} to calculate the AIC of the three
models above. Which one is the best? Why is this question a trap?

\worksheetexercise
Using all of the various techniques in this workshop, try to model the
electricity time series data from the CBE dataset.




# R Environment

```{r show_session_info, echo=TRUE, message=TRUE}
devtools::session_info()
```
